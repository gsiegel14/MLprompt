Integrating Prefect 2.0 for ML Workflow Orchestration
This guide provides detailed instructions for integrating Prefect 2.0 to orchestrate the 5-step ML workflow in your prompt optimization platform. Following these instructions will enable robust execution, tracking, and scheduling of your prompt optimization processes.
1. Prefect 2.0 Setup and Configuration
Installation and Environment Setup
First, add Prefect 2.0 to your requirements.txt:
pip install -r requirements.txt
from prefect.settings import PREFECT_API_URL, PREFECT_API_KEY
from src.app.config import settings
import os

# Configure Prefect
if settings.PREFECT_API_URL:
    os.environ["PREFECT_API_URL"] = settings.PREFECT_API_URL
if settings.PREFECT_API_KEY:
    os.environ["PREFECT_API_KEY"] = settings.PREFECT_API_KEY

# Default settings for local development
DEFAULT_QUEUE = "prompt-optimization"
DEFAULT_POOL = "prompt-pool"
DEFAULT_WORK_QUEUE_CONCURRENCY = 3

from prefect.settings import PREFECT_API_URL, PREFECT_API_KEY
from src.app.config import settings
import os

# Configure Prefect
if settings.PREFECT_API_URL:
    os.environ["PREFECT_API_URL"] = settings.PREFECT_API_URL
if settings.PREFECT_API_KEY:
    os.environ["PREFECT_API_KEY"] = settings.PREFECT_API_KEY

# Default settings for local development
DEFAULT_QUEUE = "prompt-optimization"
DEFAULT_POOL = "prompt-pool"
DEFAULT_WORK_QUEUE_CONCURRENCY = 3
from prefect.settings import PREFECT_API_URL, PREFECT_API_KEY
from src.app.config import settings
import os

# Configure Prefect
if settings.PREFECT_API_URL:
    os.environ["PREFECT_API_URL"] = settings.PREFECT_API_URL
if settings.PREFECT_API_KEY:
    os.environ["PREFECT_API_KEY"] = settings.PREFECT_API_KEY

# Default settings for local development
DEFAULT_QUEUE = "prompt-optimization"
DEFAULT_POOL = "prompt-pool"
DEFAULT_WORK_QUEUE_CONCURRENCY = 3

# src/flows/tasks/data_tasks.py
from prefect import task
import pandas as pd
import json
import os
from google.cloud import storage
from src.app.models.prompt_state import PromptState
from src.app.config import settings

@task(name="load-state", retries=2, retry_delay_seconds=30)
def load_state(system_prompt_path, output_prompt_path, dataset_path, state_path=None):
    """Load or initialize PromptState & training data"""
    # If state_path is provided, load existing state
    if state_path:
        prompt_state = PromptState.load(state_path)
    else:
        # Otherwise create a new state from the provided paths
        with open(system_prompt_path, "r") as f:
            system_prompt = f.read().strip()
        
        with open(output_prompt_path, "r") as f:
            output_prompt = f.read().strip()
            
        prompt_state = PromptState(
            system_prompt=system_prompt,
            output_prompt=output_prompt
        )
    
    # Load dataset
    dataset = pd.read_csv(dataset_path)
    
    return {
        "prompt_state": prompt_state.dict(),
        "dataset": dataset.to_dict(orient="records")
    }

@task(name="save-state", retries=2)
def save_state(prompt_state_dict, iteration, bucket_name=None):
    """Save prompt state to GCS or local file system"""
    prompt_state = PromptState(**prompt_state_dict)
    
    if bucket_name:
        path = f"gs://{bucket_name}/prompt_states/{prompt_state.id}_v{iteration}.json"
    else:
        os.makedirs("data/prompt_states", exist_ok=True)
        path = f"data/prompt_states/{prompt_state.id}_v{iteration}.json"
        
    prompt_state.save(path)
    return path

     # src/flows/tasks/evaluation_tasks.py
from prefect import task
from src.app.clients.hf_evaluator import EvaluatorService

@task(name="hf-eval-baseline", retries=2)
def hf_eval_baseline(dataset_dict, metric_names=None):
    """Evaluate baseline predictions"""
    evaluator = EvaluatorService()
    
    # Prepare predictions and references
    predictions = [item.get("model_response", "") for item in dataset_dict]
    references = [item.get("ground_truth_output", "") for item in dataset_dict]
    
    # Compute metrics
    metrics = evaluator.evaluate(predictions, references, metrics=metric_names)
    
    # Attach scores to individual examples
    for i, item in enumerate(dataset_dict):
        for metric_name, scores in metrics.items():
            if isinstance(scores, list) and i < len(scores):
                if "scores" not in item:
                    item["scores"] = {}
                item["scores"][metric_name] = scores[i]
    
    # Return both the updated dataset and the aggregate metrics
    return {
        "dataset": dataset_dict,
        "metrics": metrics
    }

@task(name="hf-eval-refined", retries=2)
def hf_eval_refined(dataset_dict, metric_names=None):
    """Evaluate refined predictions"""
    evaluator = EvaluatorService()
    
    # Prepare predictions and references
    predictions = [item.get("refined_response", "") for item in dataset_dict]
    references = [item.get("ground_truth_output", "") for item in dataset_dict]
    
    # Compute metrics
    metrics = evaluator.evaluate(predictions, references, metrics=metric_names)
    
    # Attach scores to individual examples
    for i, item in enumerate(dataset_dict):
        for metric_name, scores in metrics.items():
            if isinstance(scores, list) and i < len(scores):
                if "refined_scores" not in item:
                    item["refined_scores"] = {}
                item["refined_scores"][metric_name] = scores[i]
    
    return {
        "dataset": dataset_dict,
        "metrics": metrics
    }
    # src/flows/tasks/logging_tasks.py
from prefect import task, get_run_logger
from prefect.artifacts import create_markdown_artifact, create_table_artifact
import pandas as pd

@task(name="compare-and-log")
def compare_and_log(baseline_metrics, refined_metrics, state_dict, 
                   refined_state_dict, iteration, target_metric, 
                   target_threshold, patience, no_improve_count):
    """Compare metrics and decide whether to continue or stop"""
    logger = get_run_logger()
    
    # Create evaluation metrics table artifact
    metrics_table = pd.DataFrame({
        "Metric": list(baseline_metrics.keys()),
        "Baseline": [baseline_metrics.get(k, 0.0) for k in baseline_metrics.keys()],
        "Refined": [refined_metrics.get(k, 0.0) for k in refined_metrics.keys()]
    })
    
    create_table_artifact(
        key=f"metrics-comparison-{iteration}",
        table=metrics_table,
        description=f"Metrics comparison for iteration {iteration}"
    )
    
    # Log prompts comparison
    prompt_diff = f"""
## Prompt Comparison - Iteration {iteration}

### System Prompt Changes
**Before:**Integrating Prefect 2.0 for ML Workflow Orchestration
This guide provides detailed instructions for integrating Prefect 2.0 to orchestrate the 5-step ML workflow in your prompt optimization platform. Following these instructions will enable robust execution, tracking, and scheduling of your prompt optimization processes.
1. Prefect 2.0 Setup and Configuration
Installation and Environment Setup
First, add Prefect 2.0 to your requirements.txt:
Apply to PERECTworkfl...
prefect>=2.13.0
prefect-gcp>=0.4.0  # For Google Cloud integrations
Install dependencies:
Apply to PERECTworkfl...
Run
pip install -r requirements.txt
Create a Prefect configuration file in config/prefect_config.py:
Apply to PERECTworkfl...
from prefect.settings import PREFECT_API_URL, PREFECT_API_KEY
from src.app.config import settings
import os

# Configure Prefect
if settings.PREFECT_API_URL:
    os.environ["PREFECT_API_URL"] = settings.PREFECT_API_URL
if settings.PREFECT_API_KEY:
    os.environ["PREFECT_API_KEY"] = settings.PREFECT_API_KEY

# Default settings for local development
DEFAULT_QUEUE = "prompt-optimization"
DEFAULT_POOL = "prompt-pool"
DEFAULT_WORK_QUEUE_CONCURRENCY = 3
2. Define Core Tasks
Create task modules for each component of the workflow in the src/flows/tasks/ directory:
Data Loading Tasks
Apply to PERECTworkfl...
# src/flows/tasks/data_tasks.py
from prefect import task
import pandas as pd
import json
import os
from google.cloud import storage
from src.app.models.prompt_state import PromptState
from src.app.config import settings

@task(name="load-state", retries=2, retry_delay_seconds=30)
def load_state(system_prompt_path, output_prompt_path, dataset_path, state_path=None):
    """Load or initialize PromptState & training data"""
    # If state_path is provided, load existing state
    if state_path:
        prompt_state = PromptState.load(state_path)
    else:
        # Otherwise create a new state from the provided paths
        with open(system_prompt_path, "r") as f:
            system_prompt = f.read().strip()
        
        with open(output_prompt_path, "r") as f:
            output_prompt = f.read().strip()
            
        prompt_state = PromptState(
            system_prompt=system_prompt,
            output_prompt=output_prompt
        )
    
    # Load dataset
    dataset = pd.read_csv(dataset_path)
    
    return {
        "prompt_state": prompt_state.dict(),
        "dataset": dataset.to_dict(orient="records")
    }

@task(name="save-state", retries=2)
def save_state(prompt_state_dict, iteration, bucket_name=None):
    """Save prompt state to GCS or local file system"""
    prompt_state = PromptState(**prompt_state_dict)
    
    if bucket_name:
        path = f"gs://{bucket_name}/prompt_states/{prompt_state.id}_v{iteration}.json"
    else:
        os.makedirs("data/prompt_states", exist_ok=True)
        path = f"data/prompt_states/{prompt_state.id}_v{iteration}.json"
        
    prompt_state.save(path)
    return path
Inference Tasks
Apply to PERECTworkfl...
# src/flows/tasks/inference_tasks.py
from prefect import task
import pandas as pd
from src.app.clients.vertex_client import VertexAIClient
from src.app.models.prompt_state import PromptState

@task(name="vertex-primary-inference", retries=3, retry_delay_seconds=60)
def vertex_primary_inference(state_dict, dataset_dict, vertex_project_id, 
                           vertex_location, model_name, batch_size=10):
    """Run primary inference with LLM"""
    client = VertexAIClient(
        project_id=vertex_project_id,
        location=vertex_location,
    )
    
    prompt_state = PromptState(**state_dict)
    dataset = pd.DataFrame(dataset_dict)
    
    # Process in batches to optimize API calls
    results = []
    
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        batch_results = client.batch_predict(batch, prompt_state, model_name)
        
        # Extend the dataset with predictions
        for j, result in enumerate(batch_results):
            idx = i + j
            if idx < len(dataset_dict):
                dataset_dict[idx]["model_response"] = result
    
    return dataset_dict

@task(name="vertex-refined-inference", retries=3, retry_delay_seconds=60)
def vertex_refined_inference(refined_state_dict, dataset_dict, vertex_project_id, 
                           vertex_location, model_name, batch_size=10):
    """Run inference with refined prompts"""
    client = VertexAIClient(
        project_id=vertex_project_id,
        location=vertex_location,
    )
    
    refined_prompt_state = PromptState(**refined_state_dict)
    
    # Process in batches to optimize API calls
    for i in range(0, len(dataset_dict), batch_size):
        batch = pd.DataFrame(dataset_dict[i:i+batch_size])
        batch_results = client.batch_predict(batch, refined_prompt_state, model_name)
        
        # Extend the dataset with refined predictions
        for j, result in enumerate(batch_results):
            idx = i + j
            if idx < len(dataset_dict):
                dataset_dict[idx]["refined_response"] = result
    
    return dataset_dict
Evaluation Tasks
Apply to PERECTworkfl...
# src/flows/tasks/evaluation_tasks.py
from prefect import task
from src.app.clients.hf_evaluator import EvaluatorService

@task(name="hf-eval-baseline", retries=2)
def hf_eval_baseline(dataset_dict, metric_names=None):
    """Evaluate baseline predictions"""
    evaluator = EvaluatorService()
    
    # Prepare predictions and references
    predictions = [item.get("model_response", "") for item in dataset_dict]
    references = [item.get("ground_truth_output", "") for item in dataset_dict]
    
    # Compute metrics
    metrics = evaluator.evaluate(predictions, references, metrics=metric_names)
    
    # Attach scores to individual examples
    for i, item in enumerate(dataset_dict):
        for metric_name, scores in metrics.items():
            if isinstance(scores, list) and i < len(scores):
                if "scores" not in item:
                    item["scores"] = {}
                item["scores"][metric_name] = scores[i]
    
    # Return both the updated dataset and the aggregate metrics
    return {
        "dataset": dataset_dict,
        "metrics": metrics
    }

@task(name="hf-eval-refined", retries=2)
def hf_eval_refined(dataset_dict, metric_names=None):
    """Evaluate refined predictions"""
    evaluator = EvaluatorService()
    
    # Prepare predictions and references
    predictions = [item.get("refined_response", "") for item in dataset_dict]
    references = [item.get("ground_truth_output", "") for item in dataset_dict]
    
    # Compute metrics
    metrics = evaluator.evaluate(predictions, references, metrics=metric_names)
    
    # Attach scores to individual examples
    for i, item in enumerate(dataset_dict):
        for metric_name, scores in metrics.items():
            if isinstance(scores, list) and i < len(scores):
                if "refined_scores" not in item:
                    item["refined_scores"] = {}
                item["refined_scores"][metric_name] = scores[i]
    
    return {
        "dataset": dataset_dict,
        "metrics": metrics
    }
Optimization Tasks
Apply to PERECTworkfl...
# src/flows/tasks/optimization_tasks.py
from prefect import task
import json
from typing import List, Dict
from src.app.clients.vertex_client import VertexAIClient
from src.app.models.prompt_state import PromptState

@task(name="vertex-optimizer-refine", retries=2, retry_delay_seconds=60)
def vertex_optimizer_refine(state_dict, dataset_dict, baseline_metrics,
                          vertex_project_id, vertex_location, model_name, sample_k=5):
    """Call optimizer LLM to get new prompts"""
    client = VertexAIClient(
        project_id=vertex_project_id,
        location=vertex_location,
    )
    
    prompt_state = PromptState(**state_dict)
    
    # Select worst performing examples based on a primary metric
    # This helps focus the optimizer on the most problematic cases
    if "exact_match_score" in baseline_metrics:
        primary_metric = "exact_match_score"
    else:
        # Get first metric as default
        primary_metric = list(baseline_metrics.keys())[0]
    
    # For each example, add its score from baseline metrics
    for item in dataset_dict:
        if "scores" in item and primary_metric in item["scores"]:
            item["primary_score"] = item["scores"][primary_metric]
        else:
            item["primary_score"] = 0.0
    
    # Sort by score (ascending) and take the k worst examples
    sorted_examples = sorted(dataset_dict, key=lambda x: x.get("primary_score", 0.0))
    worst_examples = sorted_examples[:sample_k]
    
    # Format the prompt for the optimizer LLM
    optimizer_prompt = _format_optimizer_prompt(prompt_state, worst_examples, baseline_metrics)
    
    # Call the optimizer LLM
    response = client.generate_response(
        model_name=model_name,
        user_content=optimizer_prompt,
        temperature=0.7
    )
    
    # Parse the optimizer's response to get updated prompts
    try:
        optimizer_result = json.loads(response)
        new_system_prompt = optimizer_result.get("system_prompt", prompt_state.system_prompt)
        new_output_prompt = optimizer_result.get("output_prompt", prompt_state.output_prompt)
    except (json.JSONDecodeError, AttributeError) as e:
        # Fall back to existing prompts if parsing fails
        new_system_prompt = prompt_state.system_prompt
        new_output_prompt = prompt_state.output_prompt
    
    # Create new prompt state as child of current one
    new_prompt_state = PromptState(
        system_prompt=new_system_prompt,
        output_prompt=new_output_prompt,
        parent_id=prompt_state.id,
        version=prompt_state.version + 1,
        metadata={
            "baseline_metrics": baseline_metrics,
            "optimizer_model": model_name
        }
    )
    
    return new_prompt_state.dict()

def _format_optimizer_prompt(prompt_state, examples, metrics):
    """Format the input for the optimizer LLM"""
    # Implementation of formatter logic here
    prompt = f"""
You are an expert prompt engineer. Your task is to improve a system prompt and output prompt 
for an LLM based on examples and performance metrics.

Current System Prompt:
{prompt_state.system_prompt}

Current Output Prompt:
{prompt_state.output_prompt}

Performance Metrics:
{json.dumps(metrics, indent=2)}

Examples where the model struggled:
"""

    for i, example in enumerate(examples):
        prompt += f"""
Example {i+1}:
User Input: {example.get('user_input', '')}
Ground Truth: {example.get('ground_truth_output', '')}
Model Response: {example.get('model_response', '')}
Score: {example.get('primary_score', 0.0)}
"""

    prompt += """
Based on these examples and metrics, please provide improved versions of both the system prompt 
and output prompt. Format your response as a JSON object with "system_prompt" and "output_prompt" keys.
"""
    return prompt
Logging and Decision Tasks
Apply to PERECTworkfl...
# src/flows/tasks/logging_tasks.py
from prefect import task, get_run_logger
from prefect.artifacts import create_markdown_artifact, create_table_artifact
import pandas as pd

@task(name="compare-and-log")
def compare_and_log(baseline_metrics, refined_metrics, state_dict, 
                   refined_state_dict, iteration, target_metric, 
                   target_threshold, patience, no_improve_count):
    """Compare metrics and decide whether to continue or stop"""
    logger = get_run_logger()
    
    # Create evaluation metrics table artifact
    metrics_table = pd.DataFrame({
        "Metric": list(baseline_metrics.keys()),
        "Baseline": [baseline_metrics.get(k, 0.0) for k in baseline_metrics.keys()],
        "Refined": [refined_metrics.get(k, 0.0) for k in refined_metrics.keys()]
    })
    
    create_table_artifact(
        key=f"metrics-comparison-{iteration}",
        table=metrics_table,
        description=f"Metrics comparison for iteration {iteration}"
    )
    
    # Log prompts comparison
    prompt_diff = f"""
## Prompt Comparison - Iteration {iteration}

### System Prompt Changes
**Before:**
{state_dict['system_prompt']}Integrating Prefect 2.0 for ML Workflow Orchestration
This guide provides detailed instructions for integrating Prefect 2.0 to orchestrate the 5-step ML workflow in your prompt optimization platform. Following these instructions will enable robust execution, tracking, and scheduling of your prompt optimization processes.
1. Prefect 2.0 Setup and Configuration
Installation and Environment Setup
First, add Prefect 2.0 to your requirements.txt:
Apply to PERECTworkfl...
prefect>=2.13.0
prefect-gcp>=0.4.0  # For Google Cloud integrations
Install dependencies:
Apply to PERECTworkfl...
Run
pip install -r requirements.txt
Create a Prefect configuration file in config/prefect_config.py:
Apply to PERECTworkfl...
from prefect.settings import PREFECT_API_URL, PREFECT_API_KEY
from src.app.config import settings
import os

# Configure Prefect
if settings.PREFECT_API_URL:
    os.environ["PREFECT_API_URL"] = settings.PREFECT_API_URL
if settings.PREFECT_API_KEY:
    os.environ["PREFECT_API_KEY"] = settings.PREFECT_API_KEY

# Default settings for local development
DEFAULT_QUEUE = "prompt-optimization"
DEFAULT_POOL = "prompt-pool"
DEFAULT_WORK_QUEUE_CONCURRENCY = 3
2. Define Core Tasks
Create task modules for each component of the workflow in the src/flows/tasks/ directory:
Data Loading Tasks
Apply to PERECTworkfl...
# src/flows/tasks/data_tasks.py
from prefect import task
import pandas as pd
import json
import os
from google.cloud import storage
from src.app.models.prompt_state import PromptState
from src.app.config import settings

@task(name="load-state", retries=2, retry_delay_seconds=30)
def load_state(system_prompt_path, output_prompt_path, dataset_path, state_path=None):
    """Load or initialize PromptState & training data"""
    # If state_path is provided, load existing state
    if state_path:
        prompt_state = PromptState.load(state_path)
    else:
        # Otherwise create a new state from the provided paths
        with open(system_prompt_path, "r") as f:
            system_prompt = f.read().strip()
        
        with open(output_prompt_path, "r") as f:
            output_prompt = f.read().strip()
            
        prompt_state = PromptState(
            system_prompt=system_prompt,
            output_prompt=output_prompt
        )
    
    # Load dataset
    dataset = pd.read_csv(dataset_path)
    
    return {
        "prompt_state": prompt_state.dict(),
        "dataset": dataset.to_dict(orient="records")
    }

@task(name="save-state", retries=2)
def save_state(prompt_state_dict, iteration, bucket_name=None):
    """Save prompt state to GCS or local file system"""
    prompt_state = PromptState(**prompt_state_dict)
    
    if bucket_name:
        path = f"gs://{bucket_name}/prompt_states/{prompt_state.id}_v{iteration}.json"
    else:
        os.makedirs("data/prompt_states", exist_ok=True)
        path = f"data/prompt_states/{prompt_state.id}_v{iteration}.json"
        
    prompt_state.save(path)
    return path
Inference Tasks
Apply to PERECTworkfl...
# src/flows/tasks/inference_tasks.py
from prefect import task
import pandas as pd
from src.app.clients.vertex_client import VertexAIClient
from src.app.models.prompt_state import PromptState

@task(name="vertex-primary-inference", retries=3, retry_delay_seconds=60)
def vertex_primary_inference(state_dict, dataset_dict, vertex_project_id, 
                           vertex_location, model_name, batch_size=10):
    """Run primary inference with LLM"""
    client = VertexAIClient(
        project_id=vertex_project_id,
        location=vertex_location,
    )
    
    prompt_state = PromptState(**state_dict)
    dataset = pd.DataFrame(dataset_dict)
    
    # Process in batches to optimize API calls
    results = []
    
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        batch_results = client.batch_predict(batch, prompt_state, model_name)
        
        # Extend the dataset with predictions
        for j, result in enumerate(batch_results):
            idx = i + j
            if idx < len(dataset_dict):
                dataset_dict[idx]["model_response"] = result
    
    return dataset_dict

@task(name="vertex-refined-inference", retries=3, retry_delay_seconds=60)
def vertex_refined_inference(refined_state_dict, dataset_dict, vertex_project_id, 
                           vertex_location, model_name, batch_size=10):
    """Run inference with refined prompts"""
    client = VertexAIClient(
        project_id=vertex_project_id,
        location=vertex_location,
    )
    
    refined_prompt_state = PromptState(**refined_state_dict)
    
    # Process in batches to optimize API calls
    for i in range(0, len(dataset_dict), batch_size):
        batch = pd.DataFrame(dataset_dict[i:i+batch_size])
        batch_results = client.batch_predict(batch, refined_prompt_state, model_name)
        
        # Extend the dataset with refined predictions
        for j, result in enumerate(batch_results):
            idx = i + j
            if idx < len(dataset_dict):
                dataset_dict[idx]["refined_response"] = result
    
    return dataset_dict
Evaluation Tasks
Apply to PERECTworkfl...
# src/flows/tasks/evaluation_tasks.py
from prefect import task
from src.app.clients.hf_evaluator import EvaluatorService

@task(name="hf-eval-baseline", retries=2)
def hf_eval_baseline(dataset_dict, metric_names=None):
    """Evaluate baseline predictions"""
    evaluator = EvaluatorService()
    
    # Prepare predictions and references
    predictions = [item.get("model_response", "") for item in dataset_dict]
    references = [item.get("ground_truth_output", "") for item in dataset_dict]
    
    # Compute metrics
    metrics = evaluator.evaluate(predictions, references, metrics=metric_names)
    
    # Attach scores to individual examples
    for i, item in enumerate(dataset_dict):
        for metric_name, scores in metrics.items():
            if isinstance(scores, list) and i < len(scores):
                if "scores" not in item:
                    item["scores"] = {}
                item["scores"][metric_name] = scores[i]
    
    # Return both the updated dataset and the aggregate metrics
    return {
        "dataset": dataset_dict,
        "metrics": metrics
    }

@task(name="hf-eval-refined", retries=2)
def hf_eval_refined(dataset_dict, metric_names=None):
    """Evaluate refined predictions"""
    evaluator = EvaluatorService()
    
    # Prepare predictions and references
    predictions = [item.get("refined_response", "") for item in dataset_dict]
    references = [item.get("ground_truth_output", "") for item in dataset_dict]
    
    # Compute metrics
    metrics = evaluator.evaluate(predictions, references, metrics=metric_names)
    
    # Attach scores to individual examples
    for i, item in enumerate(dataset_dict):
        for metric_name, scores in metrics.items():
            if isinstance(scores, list) and i < len(scores):
                if "refined_scores" not in item:
                    item["refined_scores"] = {}
                item["refined_scores"][metric_name] = scores[i]
    
    return {
        "dataset": dataset_dict,
        "metrics": metrics
    }
Optimization Tasks
Apply to PERECTworkfl...
# src/flows/tasks/optimization_tasks.py
from prefect import task
import json
from typing import List, Dict
from src.app.clients.vertex_client import VertexAIClient
from src.app.models.prompt_state import PromptState

@task(name="vertex-optimizer-refine", retries=2, retry_delay_seconds=60)
def vertex_optimizer_refine(state_dict, dataset_dict, baseline_metrics,
                          vertex_project_id, vertex_location, model_name, sample_k=5):
    """Call optimizer LLM to get new prompts"""
    client = VertexAIClient(
        project_id=vertex_project_id,
        location=vertex_location,
    )
    
    prompt_state = PromptState(**state_dict)
    
    # Select worst performing examples based on a primary metric
    # This helps focus the optimizer on the most problematic cases
    if "exact_match_score" in baseline_metrics:
        primary_metric = "exact_match_score"
    else:
        # Get first metric as default
        primary_metric = list(baseline_metrics.keys())[0]
    
    # For each example, add its score from baseline metrics
    for item in dataset_dict:
        if "scores" in item and primary_metric in item["scores"]:
            item["primary_score"] = item["scores"][primary_metric]
        else:
            item["primary_score"] = 0.0
    
    # Sort by score (ascending) and take the k worst examples
    sorted_examples = sorted(dataset_dict, key=lambda x: x.get("primary_score", 0.0))
    worst_examples = sorted_examples[:sample_k]
    
    # Format the prompt for the optimizer LLM
    optimizer_prompt = _format_optimizer_prompt(prompt_state, worst_examples, baseline_metrics)
    
    # Call the optimizer LLM
    response = client.generate_response(
        model_name=model_name,
        user_content=optimizer_prompt,
        temperature=0.7
    )
    
    # Parse the optimizer's response to get updated prompts
    try:
        optimizer_result = json.loads(response)
        new_system_prompt = optimizer_result.get("system_prompt", prompt_state.system_prompt)
        new_output_prompt = optimizer_result.get("output_prompt", prompt_state.output_prompt)
    except (json.JSONDecodeError, AttributeError) as e:
        # Fall back to existing prompts if parsing fails
        new_system_prompt = prompt_state.system_prompt
        new_output_prompt = prompt_state.output_prompt
    
    # Create new prompt state as child of current one
    new_prompt_state = PromptState(
        system_prompt=new_system_prompt,
        output_prompt=new_output_prompt,
        parent_id=prompt_state.id,
        version=prompt_state.version + 1,
        metadata={
            "baseline_metrics": baseline_metrics,
            "optimizer_model": model_name
        }
    )
    
    return new_prompt_state.dict()

def _format_optimizer_prompt(prompt_state, examples, metrics):
    """Format the input for the optimizer LLM"""
    # Implementation of formatter logic here
    prompt = f"""
You are an expert prompt engineer. Your task is to improve a system prompt and output prompt 
for an LLM based on examples and performance metrics.

Current System Prompt:
{prompt_state.system_prompt}

Current Output Prompt:
{prompt_state.output_prompt}

Performance Metrics:
{json.dumps(metrics, indent=2)}

Examples where the model struggled:
"""

    for i, example in enumerate(examples):
        prompt += f"""
Example {i+1}:
User Input: {example.get('user_input', '')}
Ground Truth: {example.get('ground_truth_output', '')}
Model Response: {example.get('model_response', '')}
Score: {example.get('primary_score', 0.0)}
"""

    prompt += """
Based on these examples and metrics, please provide improved versions of both the system prompt 
and output prompt. Format your response as a JSON object with "system_prompt" and "output_prompt" keys.
"""
    return prompt
Logging and Decision Tasks
Apply to PERECTworkfl...
# src/flows/tasks/logging_tasks.py
from prefect import task, get_run_logger
from prefect.artifacts import create_markdown_artifact, create_table_artifact
import pandas as pd

@task(name="compare-and-log")
def compare_and_log(baseline_metrics, refined_metrics, state_dict, 
                   refined_state_dict, iteration, target_metric, 
                   target_threshold, patience, no_improve_count):
    """Compare metrics and decide whether to continue or stop"""
    logger = get_run_logger()
    
    # Create evaluation metrics table artifact
    metrics_table = pd.DataFrame({
        "Metric": list(baseline_metrics.keys()),
        "Baseline": [baseline_metrics.get(k, 0.0) for k in baseline_metrics.keys()],
        "Refined": [refined_metrics.get(k, 0.0) for k in refined_metrics.keys()]
    })
    
    create_table_artifact(
        key=f"metrics-comparison-{iteration}",
        table=metrics_table,
        description=f"Metrics comparison for iteration {iteration}"
    )
    
    # Log prompts comparison
    prompt_diff = f"""
## Prompt Comparison - Iteration {iteration}

### System Prompt Changes
**Before:**
{state_dict['system_prompt']}**After:**

### Performance Metrics
| Metric | Baseline | Refined | Change |
|--------|----------|---------|--------|
"""
    for metric in baseline_metrics:
        baseline = baseline_metrics.get(metric, 0.0)
        refined = refined_metrics.get(metric, 0.0)
        change = refined - baseline
        prompt_diff += f"| {metric} | {baseline:.4f} | {refined:.4f} | {change:+.4f} |\n"
    
    create_markdown_artifact(
        key=f"prompt-diff-{iteration}",
        markdown=prompt_diff,
        description=f"Prompt changes for iteration {iteration}"
    )
    
    # Check if the target metric improved
    baseline_value = baseline_metrics.get(target_metric, 0.0)
    refined_value = refined_metrics.get(target_metric, 0.0)
    
    # Decision logic
    improved = refined_value > baseline_value
    reached_threshold = refined_value >= target_threshold
    
    if improved:
        logger.info(f"Iteration {iteration}: {target_metric} improved from {baseline_value:.4f} to {refined_value:.4f}")
        # Reset the counter if we improved
        no_improve_count = 0
        use_refined = True
    else:
        logger.info(f"Iteration {iteration}: {target_metric} did not improve ({baseline_value:.4f} â†’ {refined_value:.4f})")
        no_improve_count += 1
        use_refined = False
    
    # Check early stopping conditions
    should_stop = no_improve_count >= patience or reached_threshold
    
    return {
        "improved": improved, 
        "use_refined": use_refined,
        "should_stop": should_stop,
        "no_improve_count": no_improve_count,
        "target_value": refined_value if use_refined else baseline_value
    }# src/flows/prompt_optimization_flow.py
from prefect import flow, get_run_logger
from prefect.artifacts import create_markdown_artifact
import os
import json

from src.flows.tasks.data_tasks import load_state, save_state
from src.flows.tasks.inference_tasks import vertex_primary_inference, vertex_refined_inference
from src.flows.tasks.evaluation_tasks import hf_eval_baseline, hf_eval_refined
from src.flows.tasks.optimization_tasks import vertex_optimizer_refine
from src.flows.tasks.logging_tasks import compare_and_log
from src.app.config import settings

@flow(name="prompt-optimization-flow")
def prompt_optimization_flow(
    vertex_project_id: str = settings.VERTEX_PROJECT_ID,
    vertex_location: str = settings.VERTEX_LOCATION,
    primary_model_name: str = settings.PRIMARY_MODEL_NAME,
    optimizer_model_name: str = settings.OPTIMIZER_MODEL_NAME,
    dataset_path: str = "data/processed/train.csv",
    system_prompt_path: str = "prompts/initial_system.txt",
    output_prompt_path: str = "prompts/initial_output.txt",
    metric_names: list = ["exact_match"],
    target_metric: str = "exact_match_score",
    target_threshold: float = 0.90,
    patience: int = 3,
    max_iterations: int = 10,
    sample_k: int = 5,
    gcs_bucket_name: str = settings.GCS_BUCKET_NAME,
    state_path: str = None,
):
    """
    Main optimization flow that iteratively improves prompts using Vertex AI & HF Evaluate.
    
    This flow implements the 5-step process:
    1. Primary LLM Inference
    2. Hugging Face Evaluation
    3. Optimizer LLM
    4. Refined LLM Inference
    5. Second Evaluation
    
    Args:
        vertex_project_id: GCP project ID for Vertex AI
        vertex_location: GCP region for Vertex AI
        primary_model_name: Model for inference (e.g., gemini-1.5-flash-001)
        optimizer_model_name: Model for optimization (e.g., gemini-1.5-pro-001)
        dataset_path: Path to CSV dataset
        system_prompt_path: Path to initial system prompt file
        output_prompt_path: Path to initial output prompt file
        metric_names: List of metrics to calculate
        target_metric: Primary metric for optimization
        target_threshold: Target value for early stopping
        patience: Number of non-improving iterations before stopping
        max_iterations: Maximum number of optimization cycles
        sample_k: Number of worst examples to send to optimizer
        gcs_bucket_name: GCS bucket for artifacts
        state_path: Optional path to existing state to resume
    
    Returns:
        Dictionary with final state path and metrics
    """
    logger = get_run_logger()
    logger.info(f"Starting prompt optimization flow with target {target_metric} >= {target_threshold}")
    
    # Initialize tracking variables
    no_improve_count = 0
    best_metric_value = 0.0
    best_state_path = None
    
    # Start the optimization loop
    for iteration in range(max_iterations):
        logger.info(f"Starting iteration {iteration+1}/{max_iterations}")
        
        # 1. Load or initialize state and dataset
        state_data = load_state(
            system_prompt_path, 
            output_prompt_path, 
            dataset_path, 
            state_path
        )
        
        # 2. Run primary inference (Step 1)
        dataset_with_preds = vertex_primary_inference(
            state_data["prompt_state"],
            state_data["dataset"],
            vertex_project_id,
            vertex_location,
            primary_model_name
        )
        
        # 3. Evaluate baseline performance (Step 2)
        baseline_result = hf_eval_baseline(
            dataset_with_preds,
            metric_names
        )
        baseline_metrics = baseline_result["metrics"]
        dataset_with_scores = baseline_result["dataset"]
        
        # Log baseline metrics
        logger.info(f"Baseline metrics: {json.dumps(baseline_metrics)}")
        
        # 4. Generate refined prompts (Step 3)
        refined_prompt_state = vertex_optimizer_refine(
            state_data["prompt_state"],
            dataset_with_scores,
            baseline_metrics,
            vertex_project_id,
            vertex_location,
            optimizer_model_name,
            sample_k=sample_k
        )
        
        # 5. Run inference with refined prompts (Step 4)
        refined_dataset = vertex_refined_inference(
            refined_prompt_state,
            dataset_with_scores,
            vertex_project_id,
            vertex_location,
            primary_model_name
        )
        
        # 6. Evaluate refined performance (Step 5)
        refined_result = hf_eval_refined(
            refined_dataset,
            metric_names
        )
        refined_metrics = refined_result["metrics"]
        
        # Log refined metrics
        logger.info(f"Refined metrics: {json.dumps(refined_metrics)}")
        
        # 7. Compare and decide whether to continue
        decision = compare_and_log(
            baseline_metrics,
            refined_metrics,
            state_data["prompt_state"],
            refined_prompt_state,
            iteration+1,
            target_metric,
            target_threshold,
            patience,
            no_improve_count
        )
        
        # Update control variables
        no_improve_count = decision["no_improve_count"]
        
        # Determine which state to save
        state_to_save = refined_prompt_state if decision["use_refined"] else state_data["prompt_state"]
        state_path = save_state(state_to_save, iteration+1, gcs_bucket_name)
        
        # Track best state
        if decision["target_value"] > best_metric_value:
            best_metric_value = decision["target_value"]
            best_state_path = state_path
        
        # Check if we should stop early
        if decision["should_stop"]:
            logger.info(f"Early stopping at iteration {iteration+1}")
            break
        
        # Update state path for next iteration if using refined
        state_path = state_path if decision["use_refined"] else None
    
    # Create final summary artifact
    summary = f"""
# Prompt Optimization Summary

- Total iterations: {iteration+1}
- Best {target_metric}: {best_metric_value:.4f}
- Best prompt state: `{best_state_path}`

## Final Metrics
```json
{json.dumps(refined_metrics if decision.get("use_refined", False) else baseline_metrics, indent=2)}
```
"""
    
    create_markdown_artifact(
        key="optimization-summary",
        markdown=summary,
        description="Summary of prompt optimization results"
    )
    
    return {
        "final_state_path": best_state_path,
        "best_metric_value": best_metric_value,
        "iterations_completed": iteration+1,
        "final_metrics": refined_metrics if decision.get("use_refined", False) else baseline_metrics
    }
#!/bin/bash
# scripts/entrypoint.sh

# Default to API mode
MODE=${MODE:-api}

if [ "$MODE" = "api" ]; then
    echo "Starting FastAPI server..."
    uvicorn src.app.main:app --host 0.0.0.0 --port 8000
elif [ "$MODE" = "agent" ]; then
    echo "Starting Prefect agent..."
    python scripts/start_prefect_agent.py
elif [ "$MODE" = "deploy" ]; then
    echo "Creating Prefect deployment..."
    python scripts/create_prefect_deployment.py
else
    echo "Unknown mode: $MODE"
    echo "Valid modes: api, agent, deploy"
    exit 1
fiz
version: '3'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    env_file:
      - .env
    environment:
      - MODE=api
    
  agent:
    build: .
    volumes:
      - .:/app
    env_file:
      - .env
    environment:
      - MODE=agent
    depends_on:
      - api
      
  deploy:
    build: .
    volumes:
      - .:/app
    env_file:
      - .env
    environment:
      - MODE=deploy
    profiles:
      - deploy


from prefect import task, get_run_logger
from google.api_core.exceptions import ResourceExhausted, DeadlineExceeded

@task(
    name="vertex-inference-with-retry", 
    retries=3, 
    retry_delay_seconds=60,
    retry_jitter_factor=0.2
)
def vertex_inference_with_retry(content, model_name):
    logger = get_run_logger()
    
    try:
        # Call Vertex AI
        result = vertex_client.generate_response(model_name, content)
        return result
    except ResourceExhausted as e:
        logger.error(f"Resource exhausted error: {str(e)}")
        # This will trigger Prefect retry mechanism
        raise
    except DeadlineExceeded as e:
        logger.error(f"Deadline exceeded error: {str(e)}")
        # This will trigger Prefect retry mechanism
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        # Unexpected errors also trigger retry
        raise
# src/app/utils/telemetry.py
from prefect import task_run_logger
import time

def log_timing(func):
    """Decorator to log function timing"""
    def wrapper(*args, **kwargs):
        logger = task_run_logger()
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        duration = end_time - start_time
        logger.info(f"{func.__name__} completed in {duration:.2f} seconds")
        return result
    return wrapper

    # kubernetes/prefect-agent.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prefect-agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prefect-agent
  template:
    metadata:
      labels:
        app: prefect-agent
    spec:
      containers:
      - name: agent
        image: gcr.io/your-project/prompt-optimizer:latest
        env:
        - name: MODE
          value: "agent"
        - name: PREFECT_API_URL
          valueFrom:
            secretKeyRef:
              name: prefect-secrets
              key: api-url
        - name: PREFECT_API_KEY
          valueFrom:
            secretKeyRef:
              name: prefect-secrets
              key: api-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"

Final Notes
This implementation guide provides a comprehensive approach to integrating Prefect 2.0 for orchestrating your ML workflow. The modular task design ensures each step is individually testable and maintainable, while the overall flow provides robust error handling, resumability, and observability.
Key benefits:
Persistent state management with automatic checkpointing
Detailed logging and artifact tracking
Built-in retries and error handling
Easy API integration
Scalable deployment options
Rich visualization and monitoring through Prefect UI
The system can be extended with:
Additional metrics and evaluation strategies
More sophisticated prompt optimization techniques
A/B testing of different prompt versions
Incorporation of meta-learning and reinforcement learning as described in your ML_PERFECT_PIPELINE.md
This implementation follows the best practices outlined in your project documents and provides a solid foundation for your prompt optimization platform.
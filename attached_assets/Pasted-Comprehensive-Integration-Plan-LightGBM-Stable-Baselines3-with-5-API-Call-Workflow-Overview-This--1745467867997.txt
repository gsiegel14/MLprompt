Comprehensive Integration Plan: LightGBM & Stable-Baselines3 with 5-API Call Workflow
Overview
This plan integrates LightGBM (meta-learning predictor) and Stable-Baselines3 PPO (reinforcement learning) into your 5-API call workflow, orchestrated by Prefect 2.0 with PostgreSQL persistence.
Phase 1: Database Schema Extensions (1 week)
1.1 New PostgreSQL Tables
-- Schema extensions for ML models
CREATE TABLE meta_models (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    version INTEGER NOT NULL,
    model_path TEXT NOT NULL,
    features JSONB NOT NULL,
    metrics JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE rl_models (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    version INTEGER NOT NULL,
    model_path TEXT NOT NULL,
    actions JSONB NOT NULL,
    metrics JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE prompt_features (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    prompt_id UUID REFERENCES prompts(id),
    feature_vector JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE rl_actions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    experiment_id UUID REFERENCES experiments(id),
    prompt_id UUID REFERENCES prompts(id),
    action_type VARCHAR(50) NOT NULL,
    action_params JSONB NOT NULL,
    resulting_prompt_id UUID REFERENCES prompts(id),
    reward FLOAT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);1.2 Create Alembic Migrationpython -m src.cli create_migration "add_ml_model_tables"
python -m src.cli upgrade_db
);
how to use google cloud console and PostgreSQL

Using Google Cloud Console with PostgreSQL
Setting Up PostgreSQL on Google Cloud SQL
1. Create a PostgreSQL Instance
Access Cloud SQL
Go to Google Cloud Console
Navigate to "SQL" in the left menu or search for "SQL"
Create Instance
Click "Create Instance"
Select "PostgreSQL"
Click "Enable API" if prompted
Configure Your Instance
Enter an instance ID (e.g., prompt-optimizer-db)
Set a password for the postgres user
Choose region/zone close to your application
Select database version (PostgreSQL 14 or 15 recommended)
Choose machine type:
Dev/Testing: db-g1-small ($0.0154/hour, ~$12/month)
Production: db-n1-standard-1 or higher
Configure Storage
Start with 10GB for development
Enable automatic storage increases if needed
Configure Connections
Under "Connections" tab, choose:
Private IP (recommended for security)
Public IP with authorized networks for testing
Advanced Options
Enable backups
Set maintenance window
Enable high availability for production
Create the Instance
Click "Create" (takes 3-5 minutes to provision)
2. Configure Database Access
Create Database
Navigate to your instance
Go to "Databases" tab
Click "Create Database"
Enter name (e.g., prompt_optimizer)
Create Users
Go to "Users" tab
Click "Add User Account"
Create application user (e.g., promptopt)
Set a strong password
Optional: Create read-only users for analysts
Connecting to Your PostgreSQL Instance
1. Connect via Cloud Console
Open SQL Query Interface
Go to your instance
Click "Query" in the top menu
Execute SQL Commands
Apply to ML integrati...
   -- Example: Create schema and tables
   CREATE SCHEMA IF NOT EXISTS prompt_optimizer;
   
   CREATE TABLE prompt_optimizer.prompts (
       id UUID PRIMARY KEY,
       system_prompt TEXT NOT NULL,
       output_prompt TEXT NOT NULL,
       version INTEGER NOT NULL DEFAULT 1,
       parent_id UUID REFERENCES prompt_optimizer.prompts(id),
       metadata JSONB DEFAULT '{}'::jsonb,
       created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
   );
2. Connect via Cloud Shell
Open Cloud Shell
Click the terminal icon in the top-right of Cloud Console
Connect to PostgreSQL
Apply to ML integrati...
Run
   gcloud sql connect prompt-optimizer-db --user=postgres
   # Enter password when prompted
Run SQL Commands
Apply to ML integrati...
Run
   postgres=> \c prompt_optimizer
   prompt_optimizer=> \dt prompt_optimizer.*
3. Connect from External Applications
Install Cloud SQL Auth Proxy (for secure access)
Apply to ML integrati...
Run
   # Download
   wget https://dl.google.com/cloudsql/cloud_sql_proxy_x64.linux -O cloud_sql_proxy
   chmod +x cloud_sql_proxy
   
   # Start the proxy
   ./cloud_sql_proxy -instances=PROJECT_ID:REGION:INSTANCE_NAME=tcp:5432
Configure Your Application
Apply to ML integrati...
   # Database connection string for PostgreSQL on GCP
   DATABASE_URL=postgresql://promptopt:PASSWORD@localhost:5432/prompt_optimizer
Integration with Prefect and Your ML Platform
1. Update Docker Compose
Apply to ML integrati...
services:
  api:
    build: .
    environment:
      - DATABASE_URL=postgresql://promptopt:PASSWORD@/prompt_optimizer?host=/cloudsql/PROJECT_ID:REGION:INSTANCE_NAME
      - PREFECT_API_URL=http://prefect-api:4200/api
    volumes:
      - ./:/app
      # For Cloud SQL Auth Proxy
      - /path/to/service-account-key.json:/secrets/service-account-key.json
    command: ./start_api.sh
  
  prefect-api:
    image: prefecthq/prefect:2.13-python3.10
    environment:
      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://prefect:PASSWORD@/prefect?host=/cloudsql/PROJECT_ID:REGION:INSTANCE_NAME
    ports:
      - "4200:4200"
2. Create Start Script
Apply to ML integrati...
Run
#!/bin/bash
# start_api.sh

# Start Cloud SQL Auth Proxy in the background
./cloud_sql_proxy -instances=PROJECT_ID:REGION:INSTANCE_NAME=tcp:5432 \
  -credential_file=/secrets/service-account-key.json &

# Wait for database connection
sleep 5

# Run migrations
python -m src.cli upgrade_db

# Start application
uvicorn src.app.main:app --host 0.0.0.0 --port 8000
Best Practices for Google Cloud PostgreSQL
IAM & Security
Use service accounts with minimal permissions
Enable SSL connections
Use Private IP when possible
Regularly rotate passwords
Monitoring
Set up Cloud Monitoring alerts for:
High CPU usage
Low storage space
Replication lag
Connection count
Backups
Enable automated backups
Perform on-demand backups before major changes
Test restoration periodically
Cost Optimization
Use smallest instance size that meets your needs
Consider stopping dev/test instances when not in use
Set appropriate storage limits
Maintenance
Set maintenance windows during off-hours
Keep PostgreSQL version updated
Estimated Costs
Development Environment
db-g1-small with 10GB storage: ~$12-15/month
Networking/operations: ~$1-5/month
Production Environment
db-n1-standard-1 with 20GB storage: ~$60-75/month
High availability: +100% cost
Backups: ~$2-5/month
This setup provides a secure, scalable PostgreSQL database for your ML prompt optimization platform with proper integration with Google Cloud and Prefect.
@Perfectpost.md @README.md @API_ARCHITECTURE_README.md @APIinfo.md @APIworflow @BACKEND_IMPLEMENTATION_FRAMEWORK.md @Data.md @ML backbone @ML implementation.md @ML UI @ML_PERFECT_PIPELINE.md @MLprompt @PERECTworkflow.md @PLAN @README.md @Stage1insturctions.md @Stage2.md @Stage3.md @Stage4 

output a summary of the machine learning workflow and the setup of the tools needed to run the workflow in a production ready application. 

ML Prompt Optimization Platform: Production Workflow & Setup
Workflow Architecture
The platform implements a machine learning-driven prompt optimization loop with five core steps:
Primary LLM Inference - Generate baseline responses using Vertex AI (Gemini) with current prompts
Baseline Evaluation - Compute metrics using Hugging Face Evaluate framework
Optimizer LLM - Generate refined prompts based on performance data
Refined LLM Inference - Run inference with optimized prompts
Comparative Evaluation - Measure improvement and decide whether to loop
This cycle repeats until reaching a target metric threshold or exhausting patience/iterations.
Technology Stack
Core Components
LLM Services: Vertex AI (Gemini models)
Database: PostgreSQL for persistent storage
Workflow Orchestration: Prefect 2.0
API Layer: FastAPI
Evaluation: Hugging Face Evaluate
Monitoring: Weights & Biases
Database Implementation
PostgreSQL provides JSONB support for storing prompt metadata and metrics
Relationship model with tables for Prompts, Datasets, Experiments, and MetricsRecords
SQLAlchemy ORM with repositories for CRUD operations
Alembic for migrations and schema management
Prefect 2.0 Workflow
Apply to ML integrati...
prompt_optimization_flow
├── load_state
│   └── vertex_primary_inference → hf_eval_baseline → vertex_optimizer_refine
│       └── vertex_refined_inference → hf_eval_refined → compare_and_log
│           └── (Conditional) If metric improved or threshold not met, continue iteration
Production Setup Guide
1. Database Setup
Apply to ML integrati...
Run
# Docker Compose for PostgreSQL
docker-compose up -d db

# Initialize database
python -m src.cli init_db

# Verify connection
python -m src.cli test_connection
2. Prefect Server Configuration
Apply to ML integrati...
Run
# Install dependencies
pip install prefect psycopg2-binary

# Create PostgreSQL profile
prefect profile create postgresql-profile
prefect config set PREFECT_API_DATABASE_CONNECTION_URL="postgresql+asyncpg://prefect:prefectpass@localhost:5432/prefect"
prefect profile use postgresql-profile

# Start Prefect server
prefect server start
3. Deployment Configuration
Apply to ML integrati...
# docker-compose.yml
version: '3.8'
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: mypassword
      POSTGRES_USER: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - prompt_optimizer_network

  prefect:
    image: prefecthq/prefect:2.13-python3.10
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://postgres:mypassword@postgres:5432/prefect
      - PREFECT_API_URL=http://0.0.0.0:4200/api
    ports:
      - "4200:4200"
    command: prefect server start --host 0.0.0.0
    networks:
      - prompt_optimizer_network

  api:
    build: .
    depends_on:
      postgres:
        condition: service_healthy
      prefect:
        condition: service_started
    environment:
      - DATABASE_URL=postgresql://postgres:mypassword@postgres:5432/prompt_optimizer
      - PREFECT_API_URL=http://prefect:4200/api
      - VERTEX_PROJECT_ID=${VERTEX_PROJECT_ID}
      - VERTEX_LOCATION=${VERTEX_LOCATION}
      - PRIMARY_MODEL_NAME=${PRIMARY_MODEL_NAME}
      - OPTIMIZER_MODEL_NAME=${OPTIMIZER_MODEL_NAME}
    ports:
      - "8000:8000"
    networks:
      - prompt_optimizer_network

  prefect-agent:
    build: .
    depends_on:
      prefect:
        condition: service_started
    environment:
      - PREFECT_API_URL=http://prefect:4200/api
      - DATABASE_URL=postgresql://postgres:mypassword@postgres:5432/prompt_optimizer
    command: prefect agent start -p default-agent-pool
    networks:
      - prompt_optimizer_network

networks:
  prompt_optimizer_network:

volumes:
  postgres_data:
4. Register Prefect Deployment
Apply to ML integrati...
# prefect_deployment.py
from prefect.deployments import Deployment
from src.flows.prompt_optimization_flow import prompt_optimization_flow

deployment = Deployment.build_from_flow(
    flow=prompt_optimization_flow,
    name="prompt-optimizer",
    version="1.0",
    parameters={
        "target_metric": "exact_match_score",
        "target_threshold": 0.90,
        "max_iterations": 10
    }
)

if __name__ == "__main__":
    deployment.apply()
5. API Layer
The platform exposes the following key endpoints:
POST /api/v1/prompts - Create a new prompt
GET /api/v1/prompts/{id} - Retrieve a prompt
POST /api/v1/experiments - Create a new experiment
POST /api/v1/experiments/{id}/start - Begin execution
GET /api/v1/experiments/{id}/metrics - Retrieve results
POST /api/v1/inference/batch - Run batch inference
POST /api/v1/evaluate - Evaluate predictions
6. Advanced Components
Prompt Repository Pattern:
Version-controlled prompt management with parent-child relationships
Metadata tracking for experiment linkage
LLM Roles:
Primary LLM - Task execution
Evaluator LLM - Output assessment
Optimizer LLM - Prompt refinement
Grader LLM - Quality assurance
Environment Monitoring:
Token usage tracking
Cost accounting
Response caching
Performance metrics
ML Extensions:
Meta-learning model to predict prompt quality
RL agent for automated prompt exploration
Surrogate models for efficiency
Production Considerations
Scaling:
Horizontal API scaling via Kubernetes
PostgreSQL connection pooling
Prefect distributed execution
Security:
JWT authentication for API access
Database credential management
Environment-specific configuration
Monitoring:
Database backups via CLI tools
Experiment tracking in W&B
Cost alerts for LLM usage
GCP Integration:
Cloud SQL for PostgreSQL
Cloud Storage for artifacts
Vertex AI for model endpoints
This platform combines state-of-the-art workflow orchestration with database persistence and API accessibility to create a production-ready solution for automated prompt optimization.
what specific meta-learning model should I be using and for reinformcement? I need to figure out what ML model I will be working with. I need a plan. review the workflow. I would need the mL model to learning from the % correct on a validaiton test dataset. make some recomendiation and consider my project, need for free/low cost, but the best value and easy of use. 

Meta-Learning and Reinforcement Learning Models for Prompt Optimization
Meta-Learning Model Recommendations
Primary Recommendation: Gradient Boosting (LightGBM)
Why: Faster training than Random Forest with better performance on tabular data
Cost: Free, open-source
Features:
Low memory usage
Handle categorical features natively
Fast training even on CPU
Excellent performance on small-to-medium datasets
Apply to ML integrati...
from lightgbm import LGBMRegressor
meta_model = LGBMRegressor(n_estimators=100, learning_rate=0.05)
Alternative Options:
Scikit-learn RandomForestRegressor
Pros: Robust to overfitting, works well with default parameters
Cons: Slower training than gradient boosting methods
XGBoost
Pros: Excellent performance, industry standard
Cons: Slightly more complex parameter tuning than LightGBM
Reinforcement Learning Recommendations
Primary Recommendation: Stable-Baselines3 PPO
Why: Balance of performance and simplicity; well-suited for prompt editing actions
Cost: Free, open-source
Features:
Stable convergence with good sample efficiency
Active community and documentation
Works well with discrete action spaces (prompt edits)
Apply to ML integrati...
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# Custom environment defining prompt state, actions, rewards
env = DummyVecEnv([lambda: PromptEnvGym()])
model = PPO("MlpPolicy", env, verbose=1)
Alternative Options:
Stable-Baselines3 DQN
Pros: Better for strictly defined, discrete action spaces
Cons: Less sample efficient than PPO
Ray RLlib with PPO
Pros: Highly scalable, supports distributed training
Cons: More setup complexity, overkill for initial implementation
Implementation Plan
1. Feature Engineering for Meta-Model (2-3 days)
Extract features from prompts:
Length statistics (system_prompt, output_prompt)
Linguistic features (command verbs, question marks, structural elements)
Token counts and vocabulary statistics
Pretrained embedding averages (using sentence-transformers)
2. Meta-Model Training Pipeline (3-4 days)
Apply to ML integrati...
def train_meta_model(experiment_history_df):
    # Extract features from prompt texts
    X = extract_prompt_features(experiment_history_df)
    
    # Target is validation metric (exact_match_score)
    y = experiment_history_df['exact_match_score']
    
    # Train with cross-validation to prevent overfitting
    model = LGBMRegressor(n_estimators=100)
    cv_score = cross_val_score(model, X, y, cv=5)
    
    # Train final model on all data
    model.fit(X, y)
    
    return model
3. Custom Gym Environment for RL (4-5 days)
Apply to ML integrati...
class PromptEditEnv(gym.Env):
    def __init__(self, meta_model, initial_prompt):
        super().__init__()
        # Define action space (discrete prompt editing operations)
        self.action_space = spaces.Discrete(10)  # 10 different edit operations
        
        # Define observation space (prompt features + meta-model prediction)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(20,))
        
        self.meta_model = meta_model
        self.current_prompt = initial_prompt
    
    def step(self, action):
        # Apply edit action to current prompt
        new_prompt = self.apply_edit(self.current_prompt, action)
        
        # Use meta-model to predict quality (fast reward signal)
        features = extract_prompt_features([new_prompt])
        predicted_score = self.meta_model.predict(features)[0]
        
        # Set reward as improvement in predicted score
        reward = predicted_score - self.current_score
        
        # Update state
        self.current_prompt = new_prompt
        self.current_score = predicted_score
        
        # Get observation
        observation = self.get_observation()
        
        return observation, reward, False, {}
4. Prefect Integration Pipeline (2-3 days)
Create a Prefect task that:
Periodically retrains the meta-model with new experiment data
Runs RL training when sufficient new data exists
Integrates the RL suggestions into the optimizer prompt context
5. Evaluation Framework (2-3 days)
Develop metrics to track:
Meta-model prediction accuracy
RL agent contribution to prompt improvement
Combined performance compared to LLM-only optimization
Cost Analysis
Compute: Both models train efficiently on CPU
Meta-model: Minutes on a standard laptop
RL: 1-2 hours per training session on a standard laptop
Monthly Cost: $0 using existing infrastructure
Integration: Works with existing Prefect/PostgreSQL setup
Additional Cost: $0
Deployment: Both models are small enough to run inference within API service
Additional Cost: $0
This approach balances effectiveness with cost-consciousness by leveraging efficient open-source libraries that can run on modest hardware while still providing substantial improvements to your prompt optimization workflow.
@workflow 

 @Perfectpost.md @README.md @API_ARCHITECTURE_README.md @APIinfo.md @APIworflow @BACKEND_IMPLEMENTATION_FRAMEWORK.md @Data.md @ML backbone @ML implementation.md @ML UI @ML_PERFECT_PIPELINE.md @MLprompt @PERECTworkflow.md @PLAN @README.md @Stage1insturctions.md @Stage2.md @Stage3.md @Stage4 @workflow @optimizer |


review my data and project, specific workflow. I need a plan to integrate my table-Baselines3 PPO and Gradient Boosting (LightGBM) with my 5 API call workflow and Perfect 2.o and PostgreSQL. Build me the most comprehensive plan for accomplishing this that I can copy paste in one. 

.
Comprehensive Integration Plan: LightGBM & Stable-Baselines3 with 5-API Call Workflow
Overview
This plan integrates LightGBM (meta-learning predictor) and Stable-Baselines3 PPO (reinforcement learning) into your 5-API call workflow, orchestrated by Prefect 2.0 with PostgreSQL persistence.
Phase 1: Database Schema Extensions (1 week)
1.1 New PostgreSQL Tables
Apply to ML integrati...
-- Schema extensions for ML models
CREATE TABLE meta_models (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    version INTEGER NOT NULL,
    model_path TEXT NOT NULL,
    features JSONB NOT NULL,
    metrics JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE rl_models (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    version INTEGER NOT NULL,
    model_path TEXT NOT NULL,
    actions JSONB NOT NULL,
    metrics JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE prompt_features (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    prompt_id UUID REFERENCES prompts(id),
    feature_vector JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE rl_actions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    experiment_id UUID REFERENCES experiments(id),
    prompt_id UUID REFERENCES prompts(id),
    action_type VARCHAR(50) NOT NULL,
    action_params JSONB NOT NULL,
    resulting_prompt_id UUID REFERENCES prompts(id),
    reward FLOAT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
1.2 Create Alembic Migration
Apply to ML integrati...
Run
python -m src.cli create_migration "add_ml_model_tables"
python -m src.cli upgrade_db
1.3 Update SQLAlchemy Models

# src/app/models/database_models.py
class MetaModel(Base):
    __tablename__ = "meta_models"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    version = Column(Integer, nullable=False)
    model_path = Column(Text, nullable=False)
    features = Column(JSONB, nullable=False)
    metrics = Column(JSONB, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

class RLModel(Base):
    __tablename__ = "rl_models"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    version = Column(Integer, nullable=False)
    model_path = Column(Text, nullable=False)
    actions = Column(JSONB, nullable=False)
    metrics = Column(JSONB, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

class PromptFeatures(Base):
    __tablename__ = "prompt_features"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=False)
    feature_vector = Column(JSONB, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationship
    prompt = relationship("Prompt", backref="features")

class RLAction(Base):
    __tablename__ = "rl_actions"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    experiment_id = Column(UUID(as_uuid=True), ForeignKey("experiments.id"), nullable=False)
    prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=False)
    action_type = Column(String(50), nullable=False)
    action_params = Column(JSONB, nullable=False)
    resulting_prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=True)
    reward = Column(Float, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    experiment = relationship("Experiment", backref="rl_actions")
    prompt = relationship("Prompt", foreign_keys=[prompt_id], backref="rl_actions_source")
    resulting_prompt = relationship("Prompt", foreign_keys=[resulting_prompt_id], backref="rl_actions_result")


    Phase 3: Meta-Learning Implementation (1 week)
3.1 LightGBM Service
# src/app/ml/meta_learning.py
import os
import uuid
import json
import numpy as np
import pandas as pd
import lightgbm as lgb
from typing import Dict, List, Any, Optional, Tuple
from sklearn.model_selection import train_test_split
import joblib

class MetaLearningService:
    def __init__(self, default_target="exact_match_score"):
        self.default_target = default_target
        
    def train_model(self, dataset: List[Dict[str, Any]], 
                   target_metric: Optional[str] = None,
                   test_size: float = 0.2) -> Tuple[lgb.Booster, Dict[str, Any]]:
        """Train a LightGBM model to predict prompt quality"""
        if not dataset:
            raise ValueError("Empty dataset provided for training")
            
        df = pd.DataFrame(dataset)
        target = target_metric or self.default_target
        
        if target not in df.columns:
            raise ValueError(f"Target metric {target} not found in dataset")
            
        # Separate features from target
        X = df.drop(columns=[col for col in df.columns if col.startswith("exact_") or 
                             col.startswith("bleu_") or col.startswith("rouge_")])
        y = df[target]
        
        # Create train/test split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
        
        # Train LightGBM model
        train_data = lgb.Dataset(X_train, label=y_train)
        test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
        
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'learning_rate': 0.05,
            'num_leaves': 31,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=100,
            valid_sets=[test_data],
            early_stopping_rounds=10,
            verbose_eval=False
        )
        
        # Calculate metrics
        y_pred = model.predict(X_test)
        rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))
        r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - y_test.mean()) ** 2)
        
        metrics = {
            "rmse": float(rmse),
            "r2": float(r2),
            "best_iteration": model.best_iteration,
            "feature_importance": dict(zip(X.columns, model.feature_importance()))
        }
        
        return model, metrics
    
    def save_model(self, model: lgb.Booster, metrics: Dict[str, Any], 
                  base_path: str = "models/meta") -> str:
        """Save model to disk and return path"""
        os.makedirs(base_path, exist_ok=True)
        model_id = str(uuid.uuid4())
        model_path = f"{base_path}/lgb_model_{model_id}.txt"
        metrics_path = f"{base_path}/lgb_metrics_{model_id}.json"
        
        # Save model
        model.save_model(model_path)
        
        # Save metrics
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f)
            
        return model_path
    
    def load_model(self, model_path: str) -> lgb.Booster:
        """Load model from disk"""
        return lgb.Booster(model_file=model_path)
    
    def predict(self, model: lgb.Booster, features: Dict[str, Any]) -> float:
        """Predict quality score for a prompt based on its features"""
        # Convert to DataFrame to ensure column order
        df = pd.DataFrame([features])
        return float(model.predict(df)[0])


        # src/app/repositories/meta_model_repository.py
from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import Dict, Any, List, Optional
from src.app.models.database_models import MetaModel
import uuid

class MetaModelRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def create(self, name: str, version: int, model_path: str, 
              features: Dict[str, Any], metrics: Dict[str, Any]) -> MetaModel:
        """Create a new meta-model record"""
        meta_model = MetaModel(
            name=name,
            version=version,
            model_path=model_path,
            features=features,
            metrics=metrics
        )
        
        self.db.add(meta_model)
        self.db.commit()
        self.db.refresh(meta_model)
        return meta_model
    
    def get_latest_version(self, name: str) -> Optional[MetaModel]:
        """Get the latest version of a meta-model by name"""
        return self.db.query(MetaModel).filter(
            MetaModel.name == name
        ).order_by(desc(MetaModel.version)).first()
    
    def get_by_id(self, model_id: str) -> Optional[MetaModel]:
        """Get a meta-model by ID"""
        return self.db.query(MetaModel).filter(
            MetaModel.id == uuid.UUID(model_id)
        ).first()

        how to use google cloud console and PostgreSQL

Using Google Cloud Console with PostgreSQL
Setting Up PostgreSQL on Google Cloud SQL
1. Create a PostgreSQL Instance
Access Cloud SQL
Go to Google Cloud Console
Navigate to "SQL" in the left menu or search for "SQL"
Create Instance
Click "Create Instance"
Select "PostgreSQL"
Click "Enable API" if prompted
Configure Your Instance
Enter an instance ID (e.g., prompt-optimizer-db)
Set a password for the postgres user
Choose region/zone close to your application
Select database version (PostgreSQL 14 or 15 recommended)
Choose machine type:
Dev/Testing: db-g1-small ($0.0154/hour, ~$12/month)
Production: db-n1-standard-1 or higher
Configure Storage
Start with 10GB for development
Enable automatic storage increases if needed
Configure Connections
Under "Connections" tab, choose:
Private IP (recommended for security)
Public IP with authorized networks for testing
Advanced Options
Enable backups
Set maintenance window
Enable high availability for production
Create the Instance
Click "Create" (takes 3-5 minutes to provision)
2. Configure Database Access
Create Database
Navigate to your instance
Go to "Databases" tab
Click "Create Database"
Enter name (e.g., prompt_optimizer)
Create Users
Go to "Users" tab
Click "Add User Account"
Create application user (e.g., promptopt)
Set a strong password
Optional: Create read-only users for analysts
Connecting to Your PostgreSQL Instance
1. Connect via Cloud Console
Open SQL Query Interface
Go to your instance
Click "Query" in the top menu
Execute SQL Commands
Apply to ML integrati...
   -- Example: Create schema and tables
   CREATE SCHEMA IF NOT EXISTS prompt_optimizer;
   
   CREATE TABLE prompt_optimizer.prompts (
       id UUID PRIMARY KEY,
       system_prompt TEXT NOT NULL,
       output_prompt TEXT NOT NULL,
       version INTEGER NOT NULL DEFAULT 1,
       parent_id UUID REFERENCES prompt_optimizer.prompts(id),
       metadata JSONB DEFAULT '{}'::jsonb,
       created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
   );
2. Connect via Cloud Shell
Open Cloud Shell
Click the terminal icon in the top-right of Cloud Console
Connect to PostgreSQL
Apply to ML integrati...
Run
   gcloud sql connect prompt-optimizer-db --user=postgres
   # Enter password when prompted
Run SQL Commands
Apply to ML integrati...
Run
   postgres=> \c prompt_optimizer
   prompt_optimizer=> \dt prompt_optimizer.*
3. Connect from External Applications
Install Cloud SQL Auth Proxy (for secure access)
Apply to ML integrati...
Run
   # Download
   wget https://dl.google.com/cloudsql/cloud_sql_proxy_x64.linux -O cloud_sql_proxy
   chmod +x cloud_sql_proxy
   
   # Start the proxy
   ./cloud_sql_proxy -instances=PROJECT_ID:REGION:INSTANCE_NAME=tcp:5432
Configure Your Application
Apply to ML integrati...
   # Database connection string for PostgreSQL on GCP
   DATABASE_URL=postgresql://promptopt:PASSWORD@localhost:5432/prompt_optimizer
Integration with Prefect and Your ML Platform
1. Update Docker Compose
Apply to ML integrati...
services:
  api:
    build: .
    environment:
      - DATABASE_URL=postgresql://promptopt:PASSWORD@/prompt_optimizer?host=/cloudsql/PROJECT_ID:REGION:INSTANCE_NAME
      - PREFECT_API_URL=http://prefect-api:4200/api
    volumes:
      - ./:/app
      # For Cloud SQL Auth Proxy
      - /path/to/service-account-key.json:/secrets/service-account-key.json
    command: ./start_api.sh
  
  prefect-api:
    image: prefecthq/prefect:2.13-python3.10
    environment:
      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://prefect:PASSWORD@/prefect?host=/cloudsql/PROJECT_ID:REGION:INSTANCE_NAME
    ports:
      - "4200:4200"
2. Create Start Script
Apply to ML integrati...
Run
#!/bin/bash
# start_api.sh

# Start Cloud SQL Auth Proxy in the background
./cloud_sql_proxy -instances=PROJECT_ID:REGION:INSTANCE_NAME=tcp:5432 \
  -credential_file=/secrets/service-account-key.json &

# Wait for database connection
sleep 5

# Run migrations
python -m src.cli upgrade_db

# Start application
uvicorn src.app.main:app --host 0.0.0.0 --port 8000
Best Practices for Google Cloud PostgreSQL
IAM & Security
Use service accounts with minimal permissions
Enable SSL connections
Use Private IP when possible
Regularly rotate passwords
Monitoring
Set up Cloud Monitoring alerts for:
High CPU usage
Low storage space
Replication lag
Connection count
Backups
Enable automated backups
Perform on-demand backups before major changes
Test restoration periodically
Cost Optimization
Use smallest instance size that meets your needs
Consider stopping dev/test instances when not in use
Set appropriate storage limits
Maintenance
Set maintenance windows during off-hours
Keep PostgreSQL version updated
Estimated Costs
Development Environment
db-g1-small with 10GB storage: ~$12-15/month
Networking/operations: ~$1-5/month
Production Environment
db-n1-standard-1 with 20GB storage: ~$60-75/month
High availability: +100% cost
Backups: ~$2-5/month
This setup provides a secure, scalable PostgreSQL database for your ML prompt optimization platform with proper integration with Google Cloud and Prefect.
@Perfectpost.md @README.md @API_ARCHITECTURE_README.md @APIinfo.md @APIworflow @BACKEND_IMPLEMENTATION_FRAMEWORK.md @Data.md @ML backbone @ML implementation.md @ML UI @ML_PERFECT_PIPELINE.md @MLprompt @PERECTworkflow.md @PLAN @README.md @Stage1insturctions.md @Stage2.md @Stage3.md @Stage4 

output a summary of the machine learning workflow and the setup of the tools needed to run the workflow in a production ready application. 

ML Prompt Optimization Platform: Production Workflow & Setup
Workflow Architecture
The platform implements a machine learning-driven prompt optimization loop with five core steps:
Primary LLM Inference - Generate baseline responses using Vertex AI (Gemini) with current prompts
Baseline Evaluation - Compute metrics using Hugging Face Evaluate framework
Optimizer LLM - Generate refined prompts based on performance data
Refined LLM Inference - Run inference with optimized prompts
Comparative Evaluation - Measure improvement and decide whether to loop
This cycle repeats until reaching a target metric threshold or exhausting patience/iterations.
Technology Stack
Core Components
LLM Services: Vertex AI (Gemini models)
Database: PostgreSQL for persistent storage
Workflow Orchestration: Prefect 2.0
API Layer: FastAPI
Evaluation: Hugging Face Evaluate
Monitoring: Weights & Biases
Database Implementation
PostgreSQL provides JSONB support for storing prompt metadata and metrics
Relationship model with tables for Prompts, Datasets, Experiments, and MetricsRecords
SQLAlchemy ORM with repositories for CRUD operations
Alembic for migrations and schema management
Prefect 2.0 Workflow
Apply to ML integrati...
prompt_optimization_flow
├── load_state
│   └── vertex_primary_inference → hf_eval_baseline → vertex_optimizer_refine
│       └── vertex_refined_inference → hf_eval_refined → compare_and_log
│           └── (Conditional) If metric improved or threshold not met, continue iteration
Production Setup Guide
1. Database Setup
Apply to ML integrati...
Run
# Docker Compose for PostgreSQL
docker-compose up -d db

# Initialize database
python -m src.cli init_db

# Verify connection
python -m src.cli test_connection
2. Prefect Server Configuration
Apply to ML integrati...
Run
# Install dependencies
pip install prefect psycopg2-binary

# Create PostgreSQL profile
prefect profile create postgresql-profile
prefect config set PREFECT_API_DATABASE_CONNECTION_URL="postgresql+asyncpg://prefect:prefectpass@localhost:5432/prefect"
prefect profile use postgresql-profile

# Start Prefect server
prefect server start
3. Deployment Configuration
Apply to ML integrati...
# docker-compose.yml
version: '3.8'
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: mypassword
      POSTGRES_USER: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - prompt_optimizer_network

  prefect:
    image: prefecthq/prefect:2.13-python3.10
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://postgres:mypassword@postgres:5432/prefect
      - PREFECT_API_URL=http://0.0.0.0:4200/api
    ports:
      - "4200:4200"
    command: prefect server start --host 0.0.0.0
    networks:
      - prompt_optimizer_network

  api:
    build: .
    depends_on:
      postgres:
        condition: service_healthy
      prefect:
        condition: service_started
    environment:
      - DATABASE_URL=postgresql://postgres:mypassword@postgres:5432/prompt_optimizer
      - PREFECT_API_URL=http://prefect:4200/api
      - VERTEX_PROJECT_ID=${VERTEX_PROJECT_ID}
      - VERTEX_LOCATION=${VERTEX_LOCATION}
      - PRIMARY_MODEL_NAME=${PRIMARY_MODEL_NAME}
      - OPTIMIZER_MODEL_NAME=${OPTIMIZER_MODEL_NAME}
    ports:
      - "8000:8000"
    networks:
      - prompt_optimizer_network

  prefect-agent:
    build: .
    depends_on:
      prefect:
        condition: service_started
    environment:
      - PREFECT_API_URL=http://prefect:4200/api
      - DATABASE_URL=postgresql://postgres:mypassword@postgres:5432/prompt_optimizer
    command: prefect agent start -p default-agent-pool
    networks:
      - prompt_optimizer_network

networks:
  prompt_optimizer_network:

volumes:
  postgres_data:
4. Register Prefect Deployment
Apply to ML integrati...
# prefect_deployment.py
from prefect.deployments import Deployment
from src.flows.prompt_optimization_flow import prompt_optimization_flow

deployment = Deployment.build_from_flow(
    flow=prompt_optimization_flow,
    name="prompt-optimizer",
    version="1.0",
    parameters={
        "target_metric": "exact_match_score",
        "target_threshold": 0.90,
        "max_iterations": 10
    }
)

if __name__ == "__main__":
    deployment.apply()
5. API Layer
The platform exposes the following key endpoints:
POST /api/v1/prompts - Create a new prompt
GET /api/v1/prompts/{id} - Retrieve a prompt
POST /api/v1/experiments - Create a new experiment
POST /api/v1/experiments/{id}/start - Begin execution
GET /api/v1/experiments/{id}/metrics - Retrieve results
POST /api/v1/inference/batch - Run batch inference
POST /api/v1/evaluate - Evaluate predictions
6. Advanced Components
Prompt Repository Pattern:
Version-controlled prompt management with parent-child relationships
Metadata tracking for experiment linkage
LLM Roles:
Primary LLM - Task execution
Evaluator LLM - Output assessment
Optimizer LLM - Prompt refinement
Grader LLM - Quality assurance
Environment Monitoring:
Token usage tracking
Cost accounting
Response caching
Performance metrics
ML Extensions:
Meta-learning model to predict prompt quality
RL agent for automated prompt exploration
Surrogate models for efficiency
Production Considerations
Scaling:
Horizontal API scaling via Kubernetes
PostgreSQL connection pooling
Prefect distributed execution
Security:
JWT authentication for API access
Database credential management
Environment-specific configuration
Monitoring:
Database backups via CLI tools
Experiment tracking in W&B
Cost alerts for LLM usage
GCP Integration:
Cloud SQL for PostgreSQL
Cloud Storage for artifacts
Vertex AI for model endpoints
This platform combines state-of-the-art workflow orchestration with database persistence and API accessibility to create a production-ready solution for automated prompt optimization.
what specific meta-learning model should I be using and for reinformcement? I need to figure out what ML model I will be working with. I need a plan. review the workflow. I would need the mL model to learning from the % correct on a validaiton test dataset. make some recomendiation and consider my project, need for free/low cost, but the best value and easy of use. 

Meta-Learning and Reinforcement Learning Models for Prompt Optimization
Meta-Learning Model Recommendations
Primary Recommendation: Gradient Boosting (LightGBM)
Why: Faster training than Random Forest with better performance on tabular data
Cost: Free, open-source
Features:
Low memory usage
Handle categorical features natively
Fast training even on CPU
Excellent performance on small-to-medium datasets
Apply to ML integrati...
from lightgbm import LGBMRegressor
meta_model = LGBMRegressor(n_estimators=100, learning_rate=0.05)
Alternative Options:
Scikit-learn RandomForestRegressor
Pros: Robust to overfitting, works well with default parameters
Cons: Slower training than gradient boosting methods
XGBoost
Pros: Excellent performance, industry standard
Cons: Slightly more complex parameter tuning than LightGBM
Reinforcement Learning Recommendations
Primary Recommendation: Stable-Baselines3 PPO
Why: Balance of performance and simplicity; well-suited for prompt editing actions
Cost: Free, open-source
Features:
Stable convergence with good sample efficiency
Active community and documentation
Works well with discrete action spaces (prompt edits)
Apply to ML integrati...
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# Custom environment defining prompt state, actions, rewards
env = DummyVecEnv([lambda: PromptEnvGym()])
model = PPO("MlpPolicy", env, verbose=1)
Alternative Options:
Stable-Baselines3 DQN
Pros: Better for strictly defined, discrete action spaces
Cons: Less sample efficient than PPO
Ray RLlib with PPO
Pros: Highly scalable, supports distributed training
Cons: More setup complexity, overkill for initial implementation
Implementation Plan
1. Feature Engineering for Meta-Model (2-3 days)
Extract features from prompts:
Length statistics (system_prompt, output_prompt)
Linguistic features (command verbs, question marks, structural elements)
Token counts and vocabulary statistics
Pretrained embedding averages (using sentence-transformers)
2. Meta-Model Training Pipeline (3-4 days)
Apply to ML integrati...
def train_meta_model(experiment_history_df):
    # Extract features from prompt texts
    X = extract_prompt_features(experiment_history_df)
    
    # Target is validation metric (exact_match_score)
    y = experiment_history_df['exact_match_score']
    
    # Train with cross-validation to prevent overfitting
    model = LGBMRegressor(n_estimators=100)
    cv_score = cross_val_score(model, X, y, cv=5)
    
    # Train final model on all data
    model.fit(X, y)
    
    return model
3. Custom Gym Environment for RL (4-5 days)
Apply to ML integrati...
class PromptEditEnv(gym.Env):
    def __init__(self, meta_model, initial_prompt):
        super().__init__()
        # Define action space (discrete prompt editing operations)
        self.action_space = spaces.Discrete(10)  # 10 different edit operations
        
        # Define observation space (prompt features + meta-model prediction)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(20,))
        
        self.meta_model = meta_model
        self.current_prompt = initial_prompt
    
    def step(self, action):
        # Apply edit action to current prompt
        new_prompt = self.apply_edit(self.current_prompt, action)
        
        # Use meta-model to predict quality (fast reward signal)
        features = extract_prompt_features([new_prompt])
        predicted_score = self.meta_model.predict(features)[0]
        
        # Set reward as improvement in predicted score
        reward = predicted_score - self.current_score
        
        # Update state
        self.current_prompt = new_prompt
        self.current_score = predicted_score
        
        # Get observation
        observation = self.get_observation()
        
        return observation, reward, False, {}
4. Prefect Integration Pipeline (2-3 days)
Create a Prefect task that:
Periodically retrains the meta-model with new experiment data
Runs RL training when sufficient new data exists
Integrates the RL suggestions into the optimizer prompt context
5. Evaluation Framework (2-3 days)
Develop metrics to track:
Meta-model prediction accuracy
RL agent contribution to prompt improvement
Combined performance compared to LLM-only optimization
Cost Analysis
Compute: Both models train efficiently on CPU
Meta-model: Minutes on a standard laptop
RL: 1-2 hours per training session on a standard laptop
Monthly Cost: $0 using existing infrastructure
Integration: Works with existing Prefect/PostgreSQL setup
Additional Cost: $0
Deployment: Both models are small enough to run inference within API service
Additional Cost: $0
This approach balances effectiveness with cost-consciousness by leveraging efficient open-source libraries that can run on modest hardware while still providing substantial improvements to your prompt optimization workflow.
@workflow 

 @Perfectpost.md @README.md @API_ARCHITECTURE_README.md @APIinfo.md @APIworflow @BACKEND_IMPLEMENTATION_FRAMEWORK.md @Data.md @ML backbone @ML implementation.md @ML UI @ML_PERFECT_PIPELINE.md @MLprompt @PERECTworkflow.md @PLAN @README.md @Stage1insturctions.md @Stage2.md @Stage3.md @Stage4 @workflow @optimizer |


review my data and project, specific workflow. I need a plan to integrate my table-Baselines3 PPO and Gradient Boosting (LightGBM) with my 5 API call workflow and Perfect 2.o and PostgreSQL. Build me the most comprehensive plan for accomplishing this that I can copy paste in one. 

...
Comprehensive Integration Plan: LightGBM & Stable-Baselines3 with 5-API Call Workflow
Overview
This plan integrates LightGBM (meta-learning predictor) and Stable-Baselines3 PPO (reinforcement learning) into your 5-API call workflow, orchestrated by Prefect 2.0 with PostgreSQL persistence.
Phase 1: Database Schema Extensions (1 week)
1.1 New PostgreSQL Tables
Apply to ML integrati...
-- Schema extensions for ML models
CREATE TABLE meta_models (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    version INTEGER NOT NULL,
    model_path TEXT NOT NULL,
    features JSONB NOT NULL,
    metrics JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE rl_models (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    version INTEGER NOT NULL,
    model_path TEXT NOT NULL,
    actions JSONB NOT NULL,
    metrics JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE prompt_features (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    prompt_id UUID REFERENCES prompts(id),
    feature_vector JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE rl_actions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    experiment_id UUID REFERENCES experiments(id),
    prompt_id UUID REFERENCES prompts(id),
    action_type VARCHAR(50) NOT NULL,
    action_params JSONB NOT NULL,
    resulting_prompt_id UUID REFERENCES prompts(id),
    reward FLOAT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
1.2 Create Alembic Migration
Apply to ML integrati...
Run
python -m src.cli create_migration "add_ml_model_tables"
python -m src.cli upgrade_db
1.3 Update SQLAlchemy Models
Apply to ML integrati...
# src/app/models/database_models.py
class MetaModel(Base):
    __tablename__ = "meta_models"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    version = Column(Integer, nullable=False)
    model_path = Column(Text, nullable=False)
    features = Column(JSONB, nullable=False)
    metrics = Column(JSONB, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

class RLModel(Base):
    __tablename__ = "rl_models"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    version = Column(Integer, nullable=False)
    model_path = Column(Text, nullable=False)
    actions = Column(JSONB, nullable=False)
    metrics = Column(JSONB, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

class PromptFeatures(Base):
    __tablename__ = "prompt_features"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=False)
    feature_vector = Column(JSONB, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationship
    prompt = relationship("Prompt", backref="features")

class RLAction(Base):
    __tablename__ = "rl_actions"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    experiment_id = Column(UUID(as_uuid=True), ForeignKey("experiments.id"), nullable=False)
    prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=False)
    action_type = Column(String(50), nullable=False)
    action_params = Column(JSONB, nullable=False)
    resulting_prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=True)
    reward = Column(Float, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    experiment = relationship("Experiment", backref="rl_actions")
    prompt = relationship("Prompt", foreign_keys=[prompt_id], backref="rl_actions_source")
    resulting_prompt = relationship("Prompt", foreign_keys=[resulting_prompt_id], backref="rl_actions_result")
Phase 2: Feature Engineering (1 week)
2.1 Create Feature Extraction Module
Apply to ML integrati...
# src/app/ml/feature_extraction.py
import numpy as np
import re
from typing import Dict, List, Any
import nltk
from nltk.tokenize import word_tokenize
from sentence_transformers import SentenceTransformer

# Download NLTK resources
nltk.download('punkt')

class PromptFeatureExtractor:
    def __init__(self, embedding_model="all-MiniLM-L6-v2"):
        self.embedding_model = SentenceTransformer(embedding_model)
        
    def extract_features(self, system_prompt: str, output_prompt: str) -> Dict[str, Any]:
        """Extract features from prompt texts"""
        # Basic length features
        features = {
            # Length statistics
            "system_length": len(system_prompt),
            "output_length": len(output_prompt),
            "total_length": len(system_prompt) + len(output_prompt),
            
            # Word counts
            "system_word_count": len(word_tokenize(system_prompt)),
            "output_word_count": len(word_tokenize(output_prompt)),
            
            # Structural features
            "system_paragraphs": system_prompt.count('\n\n') + 1,
            "output_paragraphs": output_prompt.count('\n\n') + 1,
            "system_bullet_points": len(re.findall(r'[\n\r][\s]*[-•*][\s]', system_prompt)),
            "output_bullet_points": len(re.findall(r'[\n\r][\s]*[-•*][\s]', output_prompt)),
            
            # Command language
            "system_commands": len(re.findall(r'\b(must|should|always|never|do|don\'t|avoid)\b', 
                                              system_prompt.lower())),
            "output_commands": len(re.findall(r'\b(must|should|always|never|do|don\'t|avoid)\b', 
                                              output_prompt.lower())),
            
            # Question marks
            "system_questions": system_prompt.count('?'),
            "output_questions": output_prompt.count('?'),
            
            # Constraint language
            "system_constraints": len(re.findall(r'\b(only|exactly|precisely|specifically)\b', 
                                                system_prompt.lower())),
            "output_constraints": len(re.findall(r'\b(only|exactly|precisely|specifically)\b', 
                                                output_prompt.lower())),
        }
        
        # Get embeddings (add as flattened vector)
        system_embedding = self.embedding_model.encode(system_prompt)
        output_embedding = self.embedding_model.encode(output_prompt)
        
        # Add first 20 dimensions of each embedding for dimensionality reasons
        for i in range(min(20, len(system_embedding))):
            features[f"system_emb_{i}"] = float(system_embedding[i])
            
        for i in range(min(20, len(output_embedding))):
            features[f"output_emb_{i}"] = float(output_embedding[i])
            
        return features
2.2 Repository for Feature Storage
Apply to ML integrati...
# src/app/repositories/feature_repository.py
from sqlalchemy.orm import Session
from typing import Dict, Any, List, Optional
from src.app.models.database_models import PromptFeatures, Prompt
import uuid

class FeatureRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def create_features(self, prompt_id: str, feature_vector: Dict[str, Any]) -> PromptFeatures:
        """Store extracted features for a prompt"""
        features = PromptFeatures(
            prompt_id=uuid.UUID(prompt_id),
            feature_vector=feature_vector
        )
        
        self.db.add(features)
        self.db.commit()
        self.db.refresh(features)
        return features
    
    def get_features(self, prompt_id: str) -> Optional[Dict[str, Any]]:
        """Get features for a specific prompt"""
        features = self.db.query(PromptFeatures).filter(
            PromptFeatures.prompt_id == uuid.UUID(prompt_id)
        ).first()
        
        return features.feature_vector if features else None
    
    def get_feature_dataset(self, experiment_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get features with metrics for training meta-model"""
        query = self.db.query(
            PromptFeatures.feature_vector,
            Prompt.metadata
        ).join(
            Prompt,
            Prompt.id == PromptFeatures.prompt_id
        ).filter(
            Prompt.metadata.contains({"metrics": {}})
        )
        
        if experiment_id:
            query = query.filter(
                Prompt.metadata.contains({"experiment_id": experiment_id})
            )
            
        results = query.all()
        
        # Combine features with metrics
        dataset = []
        for features, metadata in results:
            if "metrics" in metadata and metadata["metrics"]:
                combined = features.copy()
                combined.update(metadata["metrics"])
                dataset.append(combined)
                
        return dataset
Phase 3: Meta-Learning Implementation (1 week)
3.1 LightGBM Service
Apply to ML integrati...
# src/app/ml/meta_learning.py
import os
import uuid
import json
import numpy as np
import pandas as pd
import lightgbm as lgb
from typing import Dict, List, Any, Optional, Tuple
from sklearn.model_selection import train_test_split
import joblib

class MetaLearningService:
    def __init__(self, default_target="exact_match_score"):
        self.default_target = default_target
        
    def train_model(self, dataset: List[Dict[str, Any]], 
                   target_metric: Optional[str] = None,
                   test_size: float = 0.2) -> Tuple[lgb.Booster, Dict[str, Any]]:
        """Train a LightGBM model to predict prompt quality"""
        if not dataset:
            raise ValueError("Empty dataset provided for training")
            
        df = pd.DataFrame(dataset)
        target = target_metric or self.default_target
        
        if target not in df.columns:
            raise ValueError(f"Target metric {target} not found in dataset")
            
        # Separate features from target
        X = df.drop(columns=[col for col in df.columns if col.startswith("exact_") or 
                             col.startswith("bleu_") or col.startswith("rouge_")])
        y = df[target]
        
        # Create train/test split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
        
        # Train LightGBM model
        train_data = lgb.Dataset(X_train, label=y_train)
        test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
        
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'learning_rate': 0.05,
            'num_leaves': 31,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=100,
            valid_sets=[test_data],
            early_stopping_rounds=10,
            verbose_eval=False
        )
        
        # Calculate metrics
        y_pred = model.predict(X_test)
        rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))
        r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - y_test.mean()) ** 2)
        
        metrics = {
            "rmse": float(rmse),
            "r2": float(r2),
            "best_iteration": model.best_iteration,
            "feature_importance": dict(zip(X.columns, model.feature_importance()))
        }
        
        return model, metrics
    
    def save_model(self, model: lgb.Booster, metrics: Dict[str, Any], 
                  base_path: str = "models/meta") -> str:
        """Save model to disk and return path"""
        os.makedirs(base_path, exist_ok=True)
        model_id = str(uuid.uuid4())
        model_path = f"{base_path}/lgb_model_{model_id}.txt"
        metrics_path = f"{base_path}/lgb_metrics_{model_id}.json"
        
        # Save model
        model.save_model(model_path)
        
        # Save metrics
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f)
            
        return model_path
    
    def load_model(self, model_path: str) -> lgb.Booster:
        """Load model from disk"""
        return lgb.Booster(model_file=model_path)
    
    def predict(self, model: lgb.Booster, features: Dict[str, Any]) -> float:
        """Predict quality score for a prompt based on its features"""
        # Convert to DataFrame to ensure column order
        df = pd.DataFrame([features])
        return float(model.predict(df)[0])
3.2 Meta-Model Repository
Apply to ML integrati...
# src/app/repositories/meta_model_repository.py
from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import Dict, Any, List, Optional
from src.app.models.database_models import MetaModel
import uuid

class MetaModelRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def create(self, name: str, version: int, model_path: str, 
              features: Dict[str, Any], metrics: Dict[str, Any]) -> MetaModel:
        """Create a new meta-model record"""
        meta_model = MetaModel(
            name=name,
            version=version,
            model_path=model_path,
            features=features,
            metrics=metrics
        )
        
        self.db.add(meta_model)
        self.db.commit()
        self.db.refresh(meta_model)
        return meta_model
    
    def get_latest_version(self, name: str) -> Optional[MetaModel]:
        """Get the latest version of a meta-model by name"""
        return self.db.query(MetaModel).filter(
            MetaModel.name == name
        ).order_by(desc(MetaModel.version)).first()
    
    def get_by_id(self, model_id: str) -> Optional[MetaModel]:
        """Get a meta-model by ID"""
        return self.db.query(MetaModel).filter(
            MetaModel.id == uuid.UUID(model_id)
        ).first()
Phase 4: Reinforcement Learning Implementation (2 weeks)
4.1 PromptEnv Gym Environment

# src/app/ml/rl_environment.py
import gym
from gym import spaces
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from src.app.ml.feature_extraction import PromptFeatureExtractor
import json

class PromptEditAction:
    """Defines possible edit actions for prompts"""
    ADD_CONSTRAINT = 0
    REMOVE_TEXT = 1 
    ADD_EXAMPLE = 2
    CHANGE_TONE = 3
    ADD_STEP_BY_STEP = 4
    ADD_REASONING_HINT = 5
    SHORTEN_PROMPT = 6
    CLARIFY_INSTRUCTION = 7
    REORDER_COMPONENTS = 8
    FORMAT_IMPROVEMENT = 9
    
    @staticmethod
    def get_all_actions():
        return {
            PromptEditAction.ADD_CONSTRAINT: "add_constraint",
            PromptEditAction.REMOVE_TEXT: "remove_text",
            PromptEditAction.ADD_EXAMPLE: "add_example",
            PromptEditAction.CHANGE_TONE: "change_tone",
            PromptEditAction.ADD_STEP_BY_STEP: "add_step_by_step",
            PromptEditAction.ADD_REASONING_HINT: "add_reasoning_hint",
            PromptEditAction.SHORTEN_PROMPT: "shorten_prompt",
            PromptEditAction.CLARIFY_INSTRUCTION: "clarify_instruction",
            PromptEditAction.REORDER_COMPONENTS: "reorder_components",
            PromptEditAction.FORMAT_IMPROVEMENT: "format_improvement"
        }
    
    @staticmethod
    def apply_edit(action_id: int, system_prompt: str, output_prompt: str) -> Tuple[str, str]:
        """Apply the specified edit to the prompts"""
        action_type = PromptEditAction.get_all_actions()[action_id]
        
        # Each action type has a specific implementation
        if action_type == "add_constraint":
            # Add a constraint to make the output more specific
            system_prompt += "\n\nIMPORTANT: Be extremely precise and follow instructions exactly."
            
        elif action_type == "remove_text":
            # Simplify by removing potentially distracting text
            if len(system_prompt) > 200:
                paragraphs = system_prompt.split("\n\n")
                if len(paragraphs) > 1:
                    # Remove shortest paragraph (likely to be less important)
                    shortest_idx = min(range(len(paragraphs)), key=lambda i: len(paragraphs[i]))
                    paragraphs.pop(shortest_idx)
                    system_prompt = "\n\n".join(paragraphs)
            
        elif action_type == "add_step_by_step":
            # Add step-by-step instruction
            output_prompt += "\n\nWork through this step-by-step. First understand the problem, then plan your approach, then execute."
            
        elif action_type == "add_reasoning_hint":
            # Add reasoning guidance
            system_prompt += "\n\nThink carefully about each part of your reasoning. Consider the logical implications of each step."
            
        elif action_type == "shorten_prompt":
            # Make prompts more concise
            if len(system_prompt) > 300:
                words = system_prompt.split()
                # Remove filler words and keep structure
                filler_words = ["very", "really", "simply", "just", "quite", "somewhat"]
                system_prompt = " ".join([w for w in words if w.lower() not in filler_words])
            
        elif action_type == "clarify_instruction":
            # Make instructions more explicit
            output_prompt = "Please provide a direct and specific answer. " + output_prompt
            
        elif action_type == "reorder_components":
            # Put most important instructions at beginning and end (primacy/recency)
            if "\n" in system_prompt:
                paragraphs = system_prompt.split("\n\n")
                if len(paragraphs) > 2:
                    # Move second paragraph to end for better emphasis
                    second = paragraphs.pop(1)
                    paragraphs.append(second)
                    system_prompt = "\n\n".join(paragraphs)
            
        elif action_type == "format_improvement":
            # Improve formatting with bullets or numbering
            if ":" in output_prompt and "\n-" not in output_prompt:
                parts = output_prompt.split(":")
                if len(parts) > 1:
                    items = parts[1].strip().split(". ")
                    if len(items) > 1:
                        bullet_points = "\n- " + "\n- ".join(items)
                        output_prompt = parts[0] + ":" + bullet_points
        
        # Return the edited prompts
        return system_prompt, output_prompt


class PromptEnvGym(gym.Env):
    """Gym environment for RL-based prompt optimization"""
    
    def __init__(self, meta_model=None, feature_extractor=None):
        super().__init__()
        self.meta_model = meta_model
        self.feature_extractor = feature_extractor or PromptFeatureExtractor()
        
        # Define action and observation spaces
        self.action_space = spaces.Discrete(len(PromptEditAction.get_all_actions()))
        
        # Observation space: prompt features
        # We'll use a fixed-size vector for the observation space
        # Adjust the shape based on your feature extractor's output dimension
        self.observation_space = spaces.Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(100,),  # Adjust based on actual feature dimension
            dtype=np.float32
        )
        
        # Initialize state
        self.current_system_prompt = ""
        self.current_output_prompt = ""
        self.current_score = 0.0
        self.prompt_history = []
        self.action_history = []
        self.reward_history = []
        
    def reset(self, system_prompt=None, output_prompt=None):
        """Reset environment with optional initial prompts"""
        # Set initial prompts
        self.current_system_prompt = system_prompt or "You are a helpful assistant."
        self.current_output_prompt = output_prompt or "Please provide a detailed response."
        
        # Reset histories
        self.prompt_history = [(self.current_system_prompt, self.current_output_prompt)]
        self.action_history = []
        self.reward_history = []
        
        # Get initial features and predicted score
        features = self.feature_extractor.extract_features(
            self.current_system_prompt, 
            self.current_output_prompt
        )
        
        if self.meta_model:
            self.current_score = self.meta_model.predict([features])[0]
        else:
            self.current_score = 0.0
            
        # Return observation
        return self._get_observation()
    
    def step(self, action):
        """Take an action to edit the prompt"""
        # Apply the action
        new_system_prompt, new_output_prompt = PromptEditAction.apply_edit(
            action, 
            self.current_system_prompt,
            self.current_output_prompt
        )
        
        # Extract features
        features = self.feature_extractor.extract_features(
            new_system_prompt, 
            new_output_prompt
        )
        
        # Get predicted score
        if self.meta_model:
            new_score = self.meta_model.predict([features])[0]
        else:
            # Without a meta-model, use heuristics (e.g., clarity, conciseness)
            # This is a simplistic placeholder
            clarity = min(1.0, len(new_system_prompt) / max(1, len(self.current_system_prompt)))
            conciseness = min(1.0, 1000 / max(1, len(new_system_prompt) + len(new_output_prompt)))
            new_score = 0.5 * clarity + 0.5 * conciseness
        
        # Calculate reward (improvement in score)
        reward = new_score - self.current_score
        
        # Update state
        self.current_system_prompt = new_system_prompt
        self.current_output_prompt = new_output_prompt
        self.current_score = new_score
        
        # Update histories
        self.prompt_history.append((new_system_prompt, new_output_prompt))
        self.action_history.append(action)
        self.reward_history.append(reward)
        
        # Check if episode is done (max steps will be handled by the training loop)
        done = False
        
        # Return observation, reward, done, info
        return self._get_observation(), reward, done, {"score": new_score}
    
    def _get_observation(self):
        """Convert current state to observation vector"""
        features = self.feature_extractor.extract_features(
            self.current_system_prompt,
            self.current_output_prompt
        )
        
        # Convert dict to fixed-size vector
        # This is a simplistic approach; you may need a more sophisticated method
        feature_values = list(features.values())
        
        # Pad or truncate to match observation space
        obs_size = self.observation_space.shape[0]
        if len(feature_values) < obs_size:
            # Pad with zeros
            feature_values.extend([0.0] * (obs_size - len(feature_values)))
        else:
            # Truncate
            feature_values = feature_values[:obs_size]
            
        return np.array(feature_values, dtype=np.float32)
    
    def get_best_prompts(self, top_k=3):
        """Return the top-k prompts based on predicted score"""
        # Sort prompt history by corresponding reward
        scores = [0] + self.reward_history  # Initial score + rewards
        cumulative_scores = np.cumsum(scores)
        
        # Get indices of top-k scores
        top_indices = np.argsort(cumulative_scores)[-top_k:]
        
        # Return top-k prompts
        return [self.prompt_history[i] for i in top_indices]

        # src/app/ml/rl_environment.py
import gym
from gym import spaces
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from src.app.ml.feature_extraction import PromptFeatureExtractor
import json

class PromptEditAction:
    """Defines possible edit actions for prompts"""
    ADD_CONSTRAINT = 0
    REMOVE_TEXT = 1 
    ADD_EXAMPLE = 2
    CHANGE_TONE = 3
    ADD_STEP_BY_STEP = 4
    ADD_REASONING_HINT = 5
    SHORTEN_PROMPT = 6
    CLARIFY_INSTRUCTION = 7
    REORDER_COMPONENTS = 8
    FORMAT_IMPROVEMENT = 9
    
    @staticmethod
    def get_all_actions():
        return {
            PromptEditAction.ADD_CONSTRAINT: "add_constraint",
            PromptEditAction.REMOVE_TEXT: "remove_text",
            PromptEditAction.ADD_EXAMPLE: "add_example",
            PromptEditAction.CHANGE_TONE: "change_tone",
            PromptEditAction.ADD_STEP_BY_STEP: "add_step_by_step",
            PromptEditAction.ADD_REASONING_HINT: "add_reasoning_hint",
            PromptEditAction.SHORTEN_PROMPT: "shorten_prompt",
            PromptEditAction.CLARIFY_INSTRUCTION: "clarify_instruction",
            PromptEditAction.REORDER_COMPONENTS: "reorder_components",
            PromptEditAction.FORMAT_IMPROVEMENT: "format_improvement"
        }
    
    @staticmethod
    def apply_edit(action_id: int, system_prompt: str, output_prompt: str) -> Tuple[str, str]:
        """Apply the specified edit to the prompts"""
        action_type = PromptEditAction.get_all_actions()[action_id]
        
        # Each action type has a specific implementation
        if action_type == "add_constraint":
            # Add a constraint to make the output more specific
            system_prompt += "\n\nIMPORTANT: Be extremely precise and follow instructions exactly."
            
        elif action_type == "remove_text":
            # Simplify by removing potentially distracting text
            if len(system_prompt) > 200:
                paragraphs = system_prompt.split("\n\n")
                if len(paragraphs) > 1:
                    # Remove shortest paragraph (likely to be less important)
                    shortest_idx = min(range(len(paragraphs)), key=lambda i: len(paragraphs[i]))
                    paragraphs.pop(shortest_idx)
                    system_prompt = "\n\n".join(paragraphs)
            
        elif action_type == "add_step_by_step":
            # Add step-by-step instruction
            output_prompt += "\n\nWork through this step-by-step. First understand the problem, then plan your approach, then execute."
            
        elif action_type == "add_reasoning_hint":
            # Add reasoning guidance
            system_prompt += "\n\nThink carefully about each part of your reasoning. Consider the logical implications of each step."
            
        elif action_type == "shorten_prompt":
            # Make prompts more concise
            if len(system_prompt) > 300:
                words = system_prompt.split()
                # Remove filler words and keep structure
                filler_words = ["very", "really", "simply", "just", "quite", "somewhat"]
                system_prompt = " ".join([w for w in words if w.lower() not in filler_words])
            
        elif action_type == "clarify_instruction":
            # Make instructions more explicit
            output_prompt = "Please provide a direct and specific answer. " + output_prompt
            
        elif action_type == "reorder_components":
            # Put most important instructions at beginning and end (primacy/recency)
            if "\n" in system_prompt:
                paragraphs = system_prompt.split("\n\n")
                if len(paragraphs) > 2:
                    # Move second paragraph to end for better emphasis
                    second = paragraphs.pop(1)
                    paragraphs.append(second)
                    system_prompt = "\n\n".join(paragraphs)
            
        elif action_type == "format_improvement":
            # Improve formatting with bullets or numbering
            if ":" in output_prompt and "\n-" not in output_prompt:
                parts = output_prompt.split(":")
                if len(parts) > 1:
                    items = parts[1].strip().split(". ")
                    if len(items) > 1:
                        bullet_points = "\n- " + "\n- ".join(items)
                        output_prompt = parts[0] + ":" + bullet_points
        
        # Return the edited prompts
        return system_prompt, output_prompt


class PromptEnvGym(gym.Env):
    """Gym environment for RL-based prompt optimization"""
    
    def __init__(self, meta_model=None, feature_extractor=None):
        super().__init__()
        self.meta_model = meta_model
        self.feature_extractor = feature_extractor or PromptFeatureExtractor()
        
        # Define action and observation spaces
        self.action_space = spaces.Discrete(len(PromptEditAction.get_all_actions()))
        
        # Observation space: prompt features
        # We'll use a fixed-size vector for the observation space
        # Adjust the shape based on your feature extractor's output dimension
        self.observation_space = spaces.Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(100,),  # Adjust based on actual feature dimension
            dtype=np.float32
        )
        
        # Initialize state
        self.current_system_prompt = ""
        self.current_output_prompt = ""
        self.current_score = 0.0
        self.prompt_history = []
        self.action_history = []
        self.reward_history = []
        
    def reset(self, system_prompt=None, output_prompt=None):
        """Reset environment with optional initial prompts"""
        # Set initial prompts
        self.current_system_prompt = system_prompt or "You are a helpful assistant."
        self.current_output_prompt = output_prompt or "Please provide a detailed response."
        
        # Reset histories
        self.prompt_history = [(self.current_system_prompt, self.current_output_prompt)]
        self.action_history = []
        self.reward_history = []
        
        # Get initial features and predicted score
        features = self.feature_extractor.extract_features(
            self.current_system_prompt, 
            self.current_output_prompt
        )
        
        if self.meta_model:
            self.current_score = self.meta_model.predict([features])[0]
        else:
            self.current_score = 0.0
            
        # Return observation
        return self._get_observation()
    
    def step(self, action):
        """Take an action to edit the prompt"""
        # Apply the action
        new_system_prompt, new_output_prompt = PromptEditAction.apply_edit(
            action, 
            self.current_system_prompt,
            self.current_output_prompt
        )
        
        # Extract features
        features = self.feature_extractor.extract_features(
            new_system_prompt, 
            new_output_prompt
        )
        
        # Get predicted score
        if self.meta_model:
            new_score = self.meta_model.predict([features])[0]
        else:
            # Without a meta-model, use heuristics (e.g., clarity, conciseness)
            # This is a simplistic placeholder
            clarity = min(1.0, len(new_system_prompt) / max(1, len(self.current_system_prompt)))
            conciseness = min(1.0, 1000 / max(1, len(new_system_prompt) + len(new_output_prompt)))
            new_score = 0.5 * clarity + 0.5 * conciseness
        
        # Calculate reward (improvement in score)
        reward = new_score - self.current_score
        
        # Update state
        self.current_system_prompt = new_system_prompt
        self.current_output_prompt = new_output_prompt
        self.current_score = new_score
        
        # Update histories
        self.prompt_history.append((new_system_prompt, new_output_prompt))
        self.action_history.append(action)
        self.reward_history.append(reward)
        
        # Check if episode is done (max steps will be handled by the training loop)
        done = False
        
        # Return observation, reward, done, info
        return self._get_observation(), reward, done, {"score": new_score}
    
    def _get_observation(self):
        """Convert current state to observation vector"""
        features = self.feature_extractor.extract_features(
            self.current_system_prompt,
            self.current_output_prompt
        )
        
        # Convert dict to fixed-size vector
        # This is a simplistic approach; you may need a more sophisticated method
        feature_values = list(features.values())
        
        # Pad or truncate to match observation space
        obs_size = self.observation_space.shape[0]
        if len(feature_values) < obs_size:
            # Pad with zeros
            feature_values.extend([0.0] * (obs_size - len(feature_values)))
        else:
            # Truncate
            feature_values = feature_values[:obs_size]
            
        return np.array(feature_values, dtype=np.float32)
    
    def get_best_prompts(self, top_k=3):
        """Return the top-k prompts based on predicted score"""
        # Sort prompt history by corresponding reward
        scores = [0] + self.reward_history  # Initial score + rewards
        cumulative_scores = np.cumsum(scores)
        
        # Get indices of top-k scores
        top_indices = np.argsort(cumulative_scores)[-top_k:]
        
        # Return top-k prompts
        return [self.prompt_history[i] for i in top_indices]

        # src/app/repositories/rl_model_repository.py
from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import Dict, Any, List, Optional
from src.app.models.database_models import RLModel
import uuid

class RLModelRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def create(self, name: str, version: int, model_path: str, 
              actions: Dict[str, Any], metrics: Dict[str, Any]) -> RLModel:
        """Create a new RL model record"""
        rl_model = RLModel(
            name=name,
            version=version,
            model_path=model_path,
            actions=actions,
            metrics=metrics
        )
        
        self.db.add(rl_model)
        self.db.commit()
        self.db.refresh(rl_model)
        return rl_model
    
    def get_latest_version(self, name: str) -> Optional[RLModel]:
        """Get the latest version of an RL model by name"""
        return self.db.query(RLModel).filter(
            RLModel.name == name
        ).order_by(desc(RLModel.version)).first()
    
    def get_by_id(self, model_id: str) -> Optional[RLModel]:
        """Get an RL model by ID"""
        return self.db.query(RLModel).filter(
            RLModel.id == uuid.UUID(model_id)
        ).first()
Perfect integration# src/flows/tasks/ml_tasks.py
from prefect import task, get_run_logger
from prefect.artifacts import create_artifact
import pandas as pd
from typing import Dict, List, Any, Optional
import uuid
import os
import json

from src.app.ml.feature_extraction import PromptFeatureExtractor
from src.app.ml.meta_learning import MetaLearningService
from src.app.ml.rl_training import RLTrainingService
from src.app.database.db import SessionLocal
from src.app.repositories.feature_repository import FeatureRepository
from src.app.repositories.meta_model_repository import MetaModelRepository
from src.app.repositories.rl_model_repository import RLModelRepository
from src.app.repositories.prompt_repository import PromptRepository

@task(name="extract-prompt-features", retries=2)
def extract_prompt_features(prompt_state_dict: Dict[str, Any]):
    """Extract features from prompt and store in database"""
    logger = get_run_logger()
    logger.info(f"Extracting features for prompt: {prompt_state_dict['id']}")
    
    # Extract features
    extractor = PromptFeatureExtractor()
    features = extractor.extract_features(
        prompt_state_dict["system_prompt"],
        prompt_state_dict["output_prompt"]
    )
    
    # Store in database
    with SessionLocal() as db:
        repo = FeatureRepository(db)
        feature_record = repo.create_features(prompt_state_dict["id"], features)
    
    return {
        "prompt_id": prompt_state_dict["id"],
        "features": features
    }

@task(name="train-meta-model", retries=2)
def train_meta_model(experiment_id: str, target_metric: str = "exact_match_score"):
    """Train a meta-model to predict prompt performance"""
    logger = get_run_logger()
    logger.info(f"Training meta-model for experiment: {experiment_id}")
    
    # Collect feature data
    with SessionLocal() as db:
        feature_repo = FeatureRepository(db)
        dataset = feature_repo.get_feature_dataset(experiment_id)
    
    if not dataset or len(dataset) < 5:
        logger.warning("Insufficient data for meta-model training, skipping")
        return None
        
    # Train model
    meta_learning = MetaLearningService(default_target=target_metric)
    model, metrics = meta_learning.train_model(dataset, target_metric)
    
    # Save model
    model_path = meta_learning.save_model(model, metrics)
    
    # Record in database
    with SessionLocal() as db:
        meta_repo = MetaModelRepository(db)
        
        # Get latest version and increment
        latest = meta_repo.get_latest_version(f"meta_model_{experiment_id}")
        version = (latest.version + 1) if latest else 1
        
        # Create record
        model_record = meta_repo.create(
            name=f"meta_model_{experiment_id}",
            version=version,
            model_path=model_path,
            features={"feature_names": list(dataset[0].keys())},
            metrics=metrics
        )
    
    # Create artifact
    create_artifact(
        key=f"meta-model-v{version}",
        description=f"Meta-model for predicting prompt quality (v{version})",
        data={
            "model_id": str(model_record.id),
            "path": model_path,
            "metrics": metrics,
            "version": version
        }
    )
    
    return {
        "model_id": str(model_record.id),
        "path": model_path,
        "metrics": metrics,
        "version": version
    }

@task(name="rl-agent-step", retries=1)
def rl_agent_step(experiment_id: str, meta_model_info: Optional[Dict[str, Any]] = None,
                prompt_id: Optional[str] = None, total_timesteps: int = 5000):
    """Train RL agent to explore prompt modifications"""
    logger = get_run_logger

    # src/app/repositories/rl_model_repository.py
from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import Dict, Any, List, Optional
from src.app.models.database_models import RLModel
import uuid

class RLModelRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def create(self, name: str, version: int, model_path: str, 
              actions: Dict[str, Any], metrics: Dict[str, Any]) -> RLModel:
        """Create a new RL model record"""
        rl_model = RLModel(
            name=name,
            version=version,
            model_path=model_path,
            actions=actions,
            metrics=metrics
        )
        
        self.db.add(rl_model)
        self.db.commit()
        self.db.refresh(rl_model)
        return rl_model
    
    def get_latest_version(self, name: str) -> Optional[RLModel]:
        """Get the latest version of an RL model by name"""
        return self.db.query(RLModel).filter(
            RLModel.name == name
        ).order_by(desc(RLModel.version)).first()
    
    def get_by_id(self, model_id: str) -> Optional[RLModel]:
        """Get an RL model by ID"""
        return self.db.query(RLModel).filter(
            RLModel.id == uuid.UUID(model_id)
        ).first()

        Simple Explanation of the ML Prompt Optimization System
What This System Does (In Plain Language)
This system helps make AI prompts better automatically, similar to how:
A cooking recipe gets better after multiple people try it and suggest improvements
A sports team reviews game footage to improve their strategy
The Main Components:
1. Database Storage
What it is: A digital filing cabinet that remembers everything
Saves all your prompts (past and current versions)
Tracks how well each prompt performed
Stores the improvement suggestions
2. The Learning System
What it is: Two AI "coaches" that work together
Coach #1 (LightGBM)
Like a talent scout who watches previous performances
Learns which prompt characteristics lead to good results
Can quickly predict "this prompt will probably work well" without testing it
Coach #2 (Reinforcement Learning)
Like a creative writing coach who tries different edits
Tests different ways to improve prompts (adding examples, changing instructions, etc.)
Remembers which types of changes worked well in the past
3. The 5-Step Workflow
Try the current prompt - Test your prompt on some questions
Measure how good it is - Calculate a score (like 75% correct)
Get improvement ideas - The system suggests better wording
Try the improved prompt - Test the new prompt on the same questions
Compare results - Keep the new version if it's better
4. The Automation System (Prefect)
What it is: Like a factory production line for prompts
Automatically runs all the testing steps in the right order
Keeps track of which version is which
Makes sure nothing gets lost if there's an interruption
What Happens Behind the Scenes:
When a prompt is tested, the system extracts "features" (like prompt length, number of examples, tone of voice)
The prediction system (LightGBM) learns patterns:
"Prompts with step-by-step instructions tend to work better"
"Prompts with too many examples confuse the AI"
The editing system (Reinforcement Learning) tries different improvements:
Add clearer instructions
Make the prompt more concise
Reorganize information
All results get stored in the database, so the system gets smarter over time
Benefits for Non-Technical Users:
No Manual Testing: You don't have to try 50 different prompt versions yourself
Data-Driven: Decisions based on actual performance, not guesswork
Continuous Learning: System gets better at improving prompts over time
Saves Work: Remembers all previous versions and why changes were made
This is similar to how Netflix learns what shows you might like, but for AI prompts instead of movies.
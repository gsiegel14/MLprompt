Next Stage of Development: Core Implementation Instructions
After reviewing the current project files, I can see we've established a solid foundation for the prompt optimization platform. We have the architecture design, Prefect workflow layout, API specifications, and implementation guidelines ready. Now it's time to move into actual code implementation.
Phase 1: Core Service Implementation
First, we need to implement the core services and models that power the optimization workflow:
Implement the PromptState model with GCS storage
Create the Vertex AI client for LLM interactions
Develop the Hugging Face evaluator service
Build the central Prefect flow orchestration
Instructions for Phase 1
As an LLM with coding knowledge building this prompt optimization platform, I'll now implement the core components that enable the 5-step workflow. I'll start with the fundamental services and models needed to execute the optimization cycle.

1. First, I'll implement the complete PromptState model in src/app/models/prompt_state.py:
   - Add full GCS integration for loading/saving prompt states
   - Implement version tracking and parent-child relationships
   - Add serialization/deserialization methods

2. Next, I'll create the Vertex AI client in src/app/clients/vertex_client.py:
   - Implement the batch_predict method for the primary inference steps
   - Develop the generate_response method for the optimizer LLM
   - Add proper error handling, retry logic, and logging

3. I'll then build the HuggingFace evaluator in src/app/clients/hf_evaluator.py:
   - Implement support for exact match, BLEU, ROUGE, and BERTScore metrics
   - Add proper caching of loaded metrics
   - Ensure robust handling of edge cases (empty strings, etc.)

4. For the Prefect workflow, I'll start by implementing the core tasks in src/flows/tasks/:
   - Create load_state task in data_tasks.py
   - Implement vertex_primary_inference and vertex_refined_inference in inference_tasks.py
   - Build hf_eval_baseline and hf_eval_refined in evaluation_tasks.py
   - Develop vertex_optimizer_refine in optimization_tasks.py
   - Create compare_and_log in logging_tasks.py

5. Finally, I'll implement the main Prefect flow in src/flows/prompt_optimization_flow.py:
   - Set up the flow parameters
   - Implement the iteration logic with early stopping
   - Add artifact creation for tracking
   - Ensure proper error handling and logging
   Phase 2: API Layer Implementation
Once the core services are implemented, we need to expose them through the API:
Create the FastAPI endpoints for prompts, inference, and experiments
Implement the Pydantic models for request/response validation
Connect API endpoints to the core services
Add authentication and security
Now that I've implemented the core services, I'll build the API layer to make the platform accessible:

1. First, I'll create the Pydantic models in src/api/models.py:
   - Define PromptCreate, PromptResponse, ExperimentCreate models
   - Implement InferenceRequest and InferenceResponse models
   - Add validation rules for each model

2. Next, I'll implement the prompt management endpoints in src/api/endpoints/prompts.py:
   - Create POST /api/v1/prompts endpoint
   - Implement GET /api/v1/prompts/{id} endpoint
   - Add PUT /api/v1/prompts/{id} endpoint

3. I'll then build the inference endpoints in src/api/endpoints/inference.py:
   - Implement POST /api/v1/inference/complete for single completion
   - Create POST /api/v1/inference/batch for batch processing
   - Add POST /api/v1/inference/evaluate for combined inference and evaluation

4. For experiment management, I'll implement in src/api/endpoints/experiments.py:
   - Create POST /api/v1/experiments endpoint
   - Add POST /api/v1/experiments/{id}/start endpoint
   - Implement GET /api/v1/experiments/{id}/metrics endpoint

5. I'll connect all routers in src/api/routers.py and set up the main FastAPI app in src/app/main.py.

6. Finally, I'll implement authentication using JWT tokens:
   - Create src/app/auth.py with token generation and verification
   - Add authentication dependencies to the API endpoints
   - Implement proper error handling for auth failures

Phase 3: Testing and Integration
With the core components and API implemented, we need to ensure everything works together:
Create unit tests for the core services
Implement integration tests for the Prefect flow
Set up end-to-end tests for the API endpoints
Create a simple CLI for local testing
To ensure the platform works correctly, I'll implement testing and integration components:

1. First, I'll create unit tests for core services:
   - Test PromptState model serialization/deserialization
   - Mock Vertex AI and test vertex_client methods
   - Test HuggingFace evaluator with sample data

2. Next, I'll implement integration tests for the Prefect flow:
   - Create test fixtures with sample data
   - Test each task individually with mocked dependencies
   - Test the full flow with a small dataset and mocked LLM responses

3. For the API, I'll add end-to-end tests:
   - Test authentication flow
   - Test prompt CRUD operations
   - Test inference endpoints with mocked services
   - Test experiment management endpoints

4. I'll create a simple CLI for local testing:
   - Implement a command-line interface in src/cli.py
   - Add commands for running the prompt optimization flow
   - Create commands for managing prompts and experiments
   - Add a command for testing inference


   Stage 2: Token Efficiency & Low-Cost Implementation
This document outlines the second phase of development for our prompt optimization platform, focusing specifically on optimizing for low cost and minimal token usage.
Review of Stage 1
In Stage 1, we established the foundational architecture:
Project structure and core components
PromptState model for version tracking
Vertex AI client for LLM interactions
HuggingFace evaluator for metrics
Prefect workflow orchestration for the 5-step optimization process
Stage 2 Focus: Token Efficiency & Cost Optimization
Our primary goals for Stage 2:
Minimize token usage in all LLM interactions
Implement cost tracking and budgeting
Add caching to eliminate redundant API calls
Optimize batch processing for efficient resource utilization
Implementation Tasks
1. Vertex AI Client Optimization# src/app/clients/vertex_client.py

class VertexAIClient:
    # Add to __init__
    def __init__(self, project_id, location):
        # Existing code...
        self.token_count = 0
        self.cost_tracker = TokenCostTracker()
        self.response_cache = {}  # Simple in-memory cache
    
    # Add token counting
    def _count_tokens(self, text):
        """Estimate token count using a simple ratio (characters/4)"""
        return len(text) // 4  # Simple estimation, replace with tiktoken or similar
    
    # Update generate_response
    def generate_response(self, model_name, user_content, system_content=None, 
                         temperature=0.0, max_tokens=1024, response_mime_type=None):
        # Create cache key
        cache_key = f"{model_name}:{hash(user_content)}:{hash(system_content)}:{temperature}:{max_tokens}"
        
        # Check cache first
        if cache_key in self.response_cache:
            return self.response_cache[cache_key]
            
        # Count input tokens
        input_tokens = self._count_tokens(user_content)
        if system_content:
            input_tokens += self._count_tokens(system_content)
            
        # Existing implementation...
        response = # call Vertex AI
        
        # Count output tokens and update tracker
        output_tokens = self._count_tokens(response)
        self.token_count += input_tokens + output_tokens
        self.cost_tracker.add_usage(model_name, input_tokens, output_tokens)
        
        # Cache result
        self.response_cache[cache_key] = response
        return response

        ocument outlines the second phase of development for our prompt optimization platform, focusing specifically on optimizing for low cost and minimal token usage.
Review of Stage 1
In Stage 1, we established the foundational architecture:
Project structure and core components
PromptState model for version tracking
Vertex AI client for LLM interactions
HuggingFace evaluator for metrics
Prefect workflow orchestration for the 5-step optimization process
Stage 2 Focus: Token Efficiency & Cost Optimization
Our primary goals for Stage 2:
Minimize token usage in all LLM interactions
Implement cost tracking and budgeting
Add caching to eliminate redundant API calls
Optimize batch processing for efficient resource utilization
Implementation Tasks
1. Vertex AI Client Optimization
Apply to Stage2.md
# src/app/clients/vertex_client.py

class VertexAIClient:
    # Add to __init__
    def __init__(self, project_id, location):
        # Existing code...
        self.token_count = 0
        self.cost_tracker = TokenCostTracker()
        self.response_cache = {}  # Simple in-memory cache
    
    # Add token counting
    def _count_tokens(self, text):
        """Estimate token count using a simple ratio (characters/4)"""
        return len(text) // 4  # Simple estimation, replace with tiktoken or similar
    
    # Update generate_response
    def generate_response(self, model_name, user_content, system_content=None, 
                         temperature=0.0, max_tokens=1024, response_mime_type=None):
        # Create cache key
        cache_key = f"{model_name}:{hash(user_content)}:{hash(system_content)}:{temperature}:{max_tokens}"
        
        # Check cache first
        if cache_key in self.response_cache:
            return self.response_cache[cache_key]
            
        # Count input tokens
        input_tokens = self._count_tokens(user_content)
        if system_content:
            input_tokens += self._count_tokens(system_content)
            
        # Existing implementation...
        response = # call Vertex AI
        
        # Count output tokens and update tracker
        output_tokens = self._count_tokens(response)
        self.token_count += input_tokens + output_tokens
        self.cost_tracker.add_usage(model_name, input_tokens, output_tokens)
        
        # Cache result
        self.response_cache[cache_key] = response
        return response
2. Optimizer Context Minimization# src/app/optimizer.py

def _select_representative_examples(self, examples, max_examples=3):
    """Select the most informative examples within token budget"""
    if len(examples) <= max_examples:
        return examples
        
    # Strategy 1: Choose worst-performing examples
    sorted_examples = sorted(examples, key=lambda x: x['score'])
    return sorted_examples[:max_examples]
    
    # Alternative: Balance between worst examples and diversity
    # (implementation depends on your specific use case)

def _format_context_for_optimizer(self, current_system_prompt, current_output_prompt, 
                                examples, metrics):
    """Format context with minimal tokens"""
    # Truncate system prompt if very long (preserve beginning and end)
    if len(current_system_prompt) > 500:
        current_system_prompt = (current_system_prompt[:200] + 
                               "\n[...truncated for brevity...]\n" + 
                               current_system_prompt[-200:])
    
    # Select minimal but representative examples
    selected_examples = self._select_representative_examples(examples)
    
    # Format examples minimally
    formatted_examples = []
    for ex in selected_examples:
        formatted_examples.append(
            f"Input: {ex['user_input'][:100]}...\n"
            f"Expected: {ex['ground_truth_output'][:100]}...\n"
            f"Actual: {ex['model_response'][:100]}...\n"
            f"Score: {ex['score']}"
        )
    
    # Include only key metrics
    key_metrics = {k: metrics[k] for k in ['exact_match_score'] 
                  if k in metrics}
    
    # Return minimized context
    # ...Efficient Task Batching in Prefect Flow# src/flows/tasks/inference_tasks.py

@task(name="vertex-batch-inference", retries=2)
def vertex_batch_inference(batch_data, state_dict, vertex_project_id, 
                          vertex_location, model_name, batch_size=10):
    """Process examples in batches to optimize API calls"""
    client = VertexAIClient(
        project_id=vertex_project_id,
        location=vertex_location,
    )
    prompt_state = PromptState(**state_dict)
    
    # Process in optimal batch sizes
    results = []
    total_examples = len(batch_data)
    
    for i in range(0, total_examples, batch_size):
        batch = batch_data[i:i+batch_size]
        # Process batch
        batch_results = client.batch_predict(batch, prompt_state, model_name)
        results.extend(batch_results)
        
    return results

    # src/app/utils/cost_tracking.py

class TokenCostTracker:
    # Model costs in USD per 1000 tokens
    PRICING = {
        "gemini-1.5-flash-001": {"input": 0.00035, "output": 0.00105},
        "gemini-1.5-pro-001": {"input": 0.0007, "output": 0.0021}
    }
    
    def __init__(self):
        self.usage = {}  # model -> {input_tokens, output_tokens, cost}
        self.total_cost = 0.0
        
    def add_usage(self, model, input_tokens, output_tokens):
        if model not in self.usage:
            self.usage[model] = {"input_tokens": 0, "output_tokens": 0, "cost": 0.0}
            
        # Add tokens
        self.usage[model]["input_tokens"] += input_tokens
        self.usage[model]["output_tokens"] += output_tokens
        
        # Calculate cost
        if model in self.PRICING:
            input_cost = (input_tokens / 1000) * self.PRICING[model]["input"]
            output_cost = (output_tokens / 1000) * self.PRICING[model]["output"]
            cost = input_cost + output_cost
            self.usage[model]["cost"] += cost
            self.total_cost += cost
            
    def get_usage_report(self):
        return {
            "models": self.usage,
            "total_cost": self.total_cost
        }
        
    def enforce_budget(self, max_budget):
        """Check if budget exceeded and raise exception if so"""
        if self.total_cost > max_budget:
            raise BudgetExceededException(
                f"Budget of ${max_budget:.2f} exceeded. Current cost: ${self.total_cost:.2f}"
            )


            Token Efficiency Strategies
Context Truncation: Limit example length and count when sending to the optimizer LLM
Caching: Cache LLM responses to avoid redundant API calls
Batching: Process examples in optimal batch sizes
Progressive Sampling: Start with small subsets and increase only if needed
Early Stopping: Halt training when a good prompt is found or budget is exceeded
Cost Tracking Implementation
Token Counting: Count tokens for all LLM interactions
Model-specific Pricing: Track costs based on the exact model used
Budget Enforcement: Stop training when budget is exceeded
Usage Reporting: Generate detailed reports of token usage and costs
Testing Focus
Test particularly these components:
Token counting accuracy compared to actual billing
Cache hit rates
Budget enforcement correctness
Optimization effectiveness with minimal examples
Next Steps for Stage 3
After implementing these token efficiency and cost control measures, Stage 3 will focus on:
API endpoint implementation
Authentication and security
Front-end interface for monitoring experiments
Deployment pipeline
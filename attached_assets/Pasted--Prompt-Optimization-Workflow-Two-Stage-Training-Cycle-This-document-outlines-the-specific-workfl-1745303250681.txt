# Prompt Optimization Workflow: Two-Stage Training Cycle

This document outlines the specific workflow used in this system for refining prompts using a two-stage process involving a Primary LLM and an Optimizer LLM within each training run, followed by a separate validation step.

## Workflow Overview

The core idea is to iteratively improve a set of `system_prompt` and `output_prompt` used by a Primary LLM. Each **Training Run** consists of two main phases executed sequentially:

1.  **Phase 1: Inference & Evaluation:** The Primary LLM processes the entire training dataset using the *current* prompts, and its outputs are evaluated against ground truth.
2.  **Phase 2: Prompt Refinement:** The Optimizer LLM analyzes the results from Phase 1 and generates *improved* versions of the `system_prompt` and `output_prompt` for the *next* training run.

After one or more Training Runs, the user can perform a separate **Validation Phase** to compare the performance of different prompt versions on unseen data.

## Detailed Workflow Steps

Here's a step-by-step breakdown:

---

### Training Run (Iterative Cycle)

**Phase 1: Primary LLM Inference & Evaluation**

1.  **Load Current Prompts:** The system loads the current `system_prompt` and `output_prompt` (e.g., `system_v1.txt`, `output_v1.txt`). For the very first run, these are the initial prompts provided by the user.
2.  **Load Training Data:** The training dataset (e.g., `train.csv`) containing `user_input` and `ground_truth_output` columns is loaded.
3.  **Iterate Through Training Data:** The system processes each row in the training dataset:
    *   **a. Extract Input:** Get the `user_input` from the current row.
    *   **b. Prepare API Call:** Construct the input for the Primary LLM (e.g., Google Gemini API) using:
        *   The loaded `system_prompt`.
        *   The combination of the row's `user_input` and the loaded `output_prompt`.
    *   **c. Call Primary LLM:** Send the prepared input to the Primary LLM API (**LLM Call #1**).
    *   **d. Record Response:** Receive the `model_response` from the Primary LLM.
    *   **e. Evaluate:** Compare the `model_response` against the `ground_truth_output` from the current row. Calculate a score (e.g., accuracy, match).
    *   **f. Store Results:** Store the `user_input`, `ground_truth_output`, `model_response`, and the calculated `score` for this row.
4.  **Complete Cycle:** Repeat step 3 for all rows in the training dataset. All results from this phase are now collected.

**Phase 2: Optimizer LLM Refinement**

5.  **Prepare Optimizer Context:** Consolidate the results collected in Phase 1 (all inputs, ground truths, model responses, scores) along with the `system_prompt` and `output_prompt` *used* in Phase 1. Format this information according to the Optimizer LLM's instructions (see `optimizer_instructions.txt`).
6.  **Call Optimizer LLM:** Send this consolidated context to the Optimizer LLM (**LLM Call #2**).
7.  **Receive Refined Prompts:** The Optimizer LLM analyzes the performance data and returns:
    *   A new, refined `system_prompt`.
    *   A new, refined `output_prompt`.
    *   Reasoning for the changes.
8.  **Save New Prompts:** Save the refined prompts (e.g., as `system_v2.txt`, `output_v2.txt`). These prompts will be used as the "Current Prompts" in the *next* Training Run (Phase 1, Step 1).

**End of Training Run:** One full pass through Phase 1 and Phase 2 constitutes a single Training Run. The user can configure the system to perform multiple consecutive Training Runs.

---

### Validation Phase (User-Initiated)

This phase is performed *separately* by the user after completing one or more Training Runs.

1.  **Select Prompts:** The user chooses which prompt versions to compare (e.g., the initial prompts vs. the prompts after 5 training runs).
2.  **Load Validation Data:** Load a *separate* validation dataset (`validation.csv`) containing `user_input` and `ground_truth_output`, which was *not* used during the Training Runs.
3.  **Run Inference:** For *each* selected prompt set:
    *   Iterate through the validation dataset.
    *   Call the **Primary LLM** (using the selected `system_prompt` and `output_prompt`) for each `user_input` in the validation set.
    *   Record the `model_response`.
4.  **Compare Performance:** Evaluate the recorded responses against the `ground_truth_output` from the validation set for each prompt set. Calculate overall metrics (e.g., accuracy) for each prompt version.
5.  **Analyze Results:** Compare the validation metrics to determine which prompt version performs better on unseen data.

---

## Key Components & Data

*   **Primary LLM:** The main LLM (e.g., Gemini) performing the task based on the system/output prompts.
*   **Optimizer LLM:** The second LLM responsible for analyzing performance and refining the primary prompts.
*   **`system_prompt`:** The core instructions for the Primary LLM. *Edited by the Optimizer LLM.*
*   **`output_prompt`:** Appended to the user input for the Primary LLM. *Edited by the Optimizer LLM.*
*   **Optimizer Instructions:** A separate, fixed prompt telling the Optimizer LLM *how* to refine the other prompts. *NOT edited during the process.*
*   **Training Data (`train.csv`):** Used during the Training Runs (Phase 1 & 2).
*   **Validation Data (`validation.csv`):** Used only during the separate Validation Phase.
*   **Run Results:** Temporary storage for `model_response` and `score` during Phase 1, used as input for Phase 2.

## Visual Representation

```mermaid
graph TD
    subgraph Training Run (Loop)
        direction TB
        StartRun[Start Training Run] --> LoadPrompts(Load Current Prompts vN);
        LoadPrompts --> LoadTrainData(Load Training Data);
        LoadTrainData --> Cycle{Loop Thru Train Data};
        Cycle -- Row --> PrepareInput(Prepare Primary LLM Input);
        PrepareInput -- system_vN, output_vN --> CallPrimary{Call Primary LLM (Call #1)};
        CallPrimary -- model_response --> Evaluate(Evaluate vs Ground Truth);
        Evaluate -- score --> StoreResult(Store Result);
        StoreResult --> Cycle;
        Cycle -- End Loop --> CollateResults(Collate All Results);
        CollateResults --> PrepOptimizerCtx(Prepare Optimizer Context);
        PrepOptimizerCtx -- All Results + Prompts vN --> CallOptimizer{Call Optimizer LLM (Call #2)};
        CallOptimizer -- system_vN+1, output_vN+1 --> SaveNewPrompts(Save New Prompts vN+1);
        SaveNewPrompts --> EndRun[End Training Run];
    end

    subgraph Validation Phase (Manual)
        direction TB
        StartVal[Start Validation] --> SelectPrompts(User Selects Prompt Versions);
        SelectPrompts --> LoadValData(Load Validation Data);
        LoadValData --> ValCycle{Loop Thru Val Data};
        ValCycle -- Row --> PrepareValInput(Prepare Primary LLM Input);
        PrepareValInput -- Selected Prompts --> CallPrimaryVal{Call Primary LLM};
        CallPrimaryVal -- response --> StoreValResp(Store Val Response);
        StoreValResp --> ValCycle;
        ValCycle -- End Loop --> EvalVal(Evaluate All Val Responses);
        EvalVal -- Metrics per Version --> CompareMetrics(User Compares Metrics);
        CompareMetrics --> EndVal[End Validation];
    end

    SaveNewPrompts -.-> |Used in Next Run| LoadPrompts;

    style CallPrimary fill:#cde4ff,stroke:#6aa8ff
    style CallOptimizer fill:#d4edda,stroke:#8fd19e
    style CallPrimaryVal fill:#cde4ff,stroke:#6aa8ff
```

This structure clarifies the two LLM calls within each training cycle and explicitly separates the user-driven validation process.
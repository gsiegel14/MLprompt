Recommendation for ML Prompt Optimization Platform: PostgreSQL
For your complex ML prompt optimization platform, I strongly recommend PostgreSQL deployed via Docker for these specific reasons:
Why PostgreSQL is Ideal for Your Case
Complex Relationships: Your platform has intricate relationships between prompts, experiments, datasets, and metrics that benefit from PostgreSQL's robust referential integrity.
JSON Support: Your metadata, metrics and versioned prompt data would benefit from PostgreSQL's native JSONB support, which allows efficient storage and querying of structured data.
Versioning Requirements: The prompt versioning system (with parent-child relationships) demands proper transactional guarantees that SQLite can't reliably provide with concurrent access.
Prefect Integration: When Prefect runs parallel flows, PostgreSQL handles concurrent database operations better than SQLite's file-locking mechanism.
Query Performance: As your metrics data grows, PostgreSQL's advanced indexing and query planning will maintain performance.
ML Workflow Integrity: Your 5-step optimization loop requires guaranteed data consistency that PostgreSQL delivers even under high concurrency.
# docker-compose.yml
   version: '3.8'
   services:
     db:
       image: postgres:15
       volumes:
         - postgres_data:/var/lib/postgresql/data
       environment:
         - POSTGRES_PASSWORD=devpassword
         - POSTGRES_USER=promptopt
         - POSTGRES_DB=promptopt
       ports:
         - "5432:5432"
   
   volumes:
     postgres_data:


     . Database Configuration

     # src/app/database/db.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from pathlib import Path

# Environment-based configuration
DATABASE_URL = os.getenv(
    "DATABASE_URL", 
    "postgresql://promptopt:devpassword@localhost:5432/promptopt"
)

# Create engine with PostgreSQL-specific configuration
engine = create_engine(
    DATABASE_URL, 
    echo=os.getenv("ENV", "development") == "development",
    pool_size=5,
    max_overflow=10,
    pool_timeout=30,
    pool_recycle=1800,  # Reconnect after 30 minutes
)

# Session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Base class for models
Base = declarative_base()

# Dependency for routes
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close(

            # src/app/models/database_models.py
from sqlalchemy import Column, Integer, String, ForeignKey, Float, DateTime, Text
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from sqlalchemy import Index
import uuid

from src.app.database.db import Base

class Prompt(Base):
    __tablename__ = "prompts"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    system_prompt = Column(Text, nullable=False)
    output_prompt = Column(Text, nullable=False)
    version = Column(Integer, default=1)
    parent_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=True)
    metadata = Column(JSONB, default={})
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    parent = relationship("Prompt", remote_side=[id], backref="versions")
    experiments = relationship("Experiment", foreign_keys="Experiment.initial_prompt_id", back_populates="initial_prompt")
    
    # Indexes
    __table_args__ = (
        Index('idx_prompt_parent_id', parent_id),
        Index('idx_prompt_created_at', created_at),
    )
    
class Dataset(Base):
    __tablename__ = "datasets"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    file_path = Column(String(1024), nullable=False)
    row_count = Column(Integer)
    columns = Column(JSONB)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    experiments = relationship("Experiment", back_populates="dataset")
    
    # Indexes
    __table_args__ = (
        Index('idx_dataset_name', name),
    )

class Experiment(Base):
    __tablename__ = "experiments"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    initial_prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=False)
    dataset_id = Column(UUID(as_uuid=True), ForeignKey("datasets.id"), nullable=False)
    metrics = Column(JSONB, default=[])
    max_epochs = Column(Integer, default=10)
    target_threshold = Column(Float, default=0.8)
    status = Column(String(50), default="created")
    best_prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    initial_prompt = relationship("Prompt", foreign_keys=[initial_prompt_id])
    best_prompt = relationship("Prompt", foreign_keys=[best_prompt_id])
    dataset = relationship("Dataset", back_populates="experiments")
    metrics_history = relationship("MetricsRecord", back_populates="experiment")
    
    # Indexes
    __table_args__ = (
        Index('idx_experiment_dataset_id', dataset_id),
        Index('idx_experiment_status', status),
    )

class MetricsRecord(Base):
    __tablename__ = "metrics_records"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    experiment_id = Column(UUID(as_uuid=True), ForeignKey("experiments.id"), nullable=False)
    epoch = Column(Integer, nullable=False)
    metrics = Column(JSONB, nullable=False)
    prompt_id = Column(UUID(as_uuid=True), ForeignKey("prompts.id"), nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    experiment = relationship("Experiment", back_populates="metrics_history")
    prompt = relationship("Prompt")
    
    # Indexes
    __table_args__ = (
        Index('idx_metrics_experiment_id', experiment_id),
        Index('idx_metrics_epoch', experiment_id, epoch),
    )

    # src/app/repositories/prompt_repository.py
from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import List, Optional, Dict, Any
from src.app.models.database_models import Prompt
from src.app.models.prompt_state import PromptState
import uuid

class PromptRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def create(self, system_prompt: str, output_prompt: str, parent_id: Optional[str] = None, 
               metadata: Optional[Dict[str, Any]] = None) -> Prompt:
        """Create a new prompt record"""
        # Get version if parent exists
        version = 1
        if parent_id:
            parent = self.db.query(Prompt).filter(Prompt.id == uuid.UUID(parent_id)).first()
            if parent:
                version = parent.version + 1
        
        # Create new prompt
        prompt = Prompt(
            system_prompt=system_prompt,
            output_prompt=output_prompt,
            parent_id=uuid.UUID(parent_id) if parent_id else None,
            version=version,
            metadata=metadata or {}
        )
        
        self.db.add(prompt)
        self.db.commit()
        self.db.refresh(prompt)
        return prompt
    
    def get_by_id(self, prompt_id: str) -> Optional[Prompt]:
        """Get prompt by ID"""
        return self.db.query(Prompt).filter(Prompt.id == uuid.UUID(prompt_id)).first()
    
    def get_latest_version(self, parent_id: str = None) -> Optional[Prompt]:
        """Get the latest version of a prompt family"""
        query = self.db.query(Prompt)
        if parent_id:
            query = query.filter(Prompt.parent_id == uuid.UUID(parent_id))
        return query.order_by(desc(Prompt.version)).first()
    
    def list_versions(self, parent_id: str) -> List[Prompt]:
        """List all versions of a prompt family"""
        return self.db.query(Prompt).filter(
            (Prompt.id == uuid.UUID(parent_id)) | (Prompt.parent_id == uuid.UUID(parent_id))
        ).order_by(Prompt.version).all()
    
    def to_prompt_state(self, prompt: Prompt) -> PromptState:
        """Convert database model to domain model"""
        return PromptState(
            system_prompt=prompt.system_prompt,
            output_prompt=prompt.output_prompt,
            id=str(prompt.id),
            parent_id=str(prompt.parent_id) if prompt.parent_id else None,
            version=prompt.version,
            metadata=prompt.metadata,
            created_at=prompt.created_at
        )

        # src/app/repositories/prompt_repository.py
from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import List, Optional, Dict, Any
from src.app.models.database_models import Prompt
from src.app.models.prompt_state import PromptState
import uuid

class PromptRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def create(self, system_prompt: str, output_prompt: str, parent_id: Optional[str] = None, 
               metadata: Optional[Dict[str, Any]] = None) -> Prompt:
        """Create a new prompt record"""
        # Get version if parent exists
        version = 1
        if parent_id:
            parent = self.db.query(Prompt).filter(Prompt.id == uuid.UUID(parent_id)).first()
            if parent:
                version = parent.version + 1
        
        # Create new prompt
        prompt = Prompt(
            system_prompt=system_prompt,
            output_prompt=output_prompt,
            parent_id=uuid.UUID(parent_id) if parent_id else None,
            version=version,
            metadata=metadata or {}
        )
        
        self.db.add(prompt)
        self.db.commit()
        self.db.refresh(prompt)
        return prompt
    
    def get_by_id(self, prompt_id: str) -> Optional[Prompt]:
        """Get prompt by ID"""
        return self.db.query(Prompt).filter(Prompt.id == uuid.UUID(prompt_id)).first()
    
    def get_latest_version(self, parent_id: str = None) -> Optional[Prompt]:
        """Get the latest version of a prompt family"""
        query = self.db.query(Prompt)
        if parent_id:
            query = query.filter(Prompt.parent_id == uuid.UUID(parent_id))
        return query.order_by(desc(Prompt.version)).first()
    
    def list_versions(self, parent_id: str) -> List[Prompt]:
        """List all versions of a prompt family"""
        return self.db.query(Prompt).filter(
            (Prompt.id == uuid.UUID(parent_id)) | (Prompt.parent_id == uuid.UUID(parent_id))
        ).order_by(Prompt.version).all()
    
    def to_prompt_state(self, prompt: Prompt) -> PromptState:
        """Convert database model to domain model"""
        return PromptState(
            system_prompt=prompt.system_prompt,
            output_prompt=prompt.output_prompt,
            id=str(prompt.id),
            parent_id=str(prompt.parent_id) if prompt.parent_id else None,
            version=prompt.version,
            metadata=prompt.metadata,
            created_at=prompt.created_at
        )
        # src/app/repositories/prompt_repository.py
from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import List, Optional, Dict, Any
from src.app.models.database_models import Prompt
from src.app.models.prompt_state import PromptState
import uuid

class PromptRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def create(self, system_prompt: str, output_prompt: str, parent_id: Optional[str] = None, 
               metadata: Optional[Dict[str, Any]] = None) -> Prompt:
        """Create a new prompt record"""
        # Get version if parent exists
        version = 1
        if parent_id:
            parent = self.db.query(Prompt).filter(Prompt.id == uuid.UUID(parent_id)).first()
            if parent:
                version = parent.version + 1
        
        # Create new prompt
        prompt = Prompt(
            system_prompt=system_prompt,
            output_prompt=output_prompt,
            parent_id=uuid.UUID(parent_id) if parent_id else None,
            version=version,
            metadata=metadata or {}
        )
        
        self.db.add(prompt)
        self.db.commit()
        self.db.refresh(prompt)
        return prompt
    
    def get_by_id(self, prompt_id: str) -> Optional[Prompt]:
        """Get prompt by ID"""
        return self.db.query(Prompt).filter(Prompt.id == uuid.UUID(prompt_id)).first()
    
    def get_latest_version(self, parent_id: str = None) -> Optional[Prompt]:
        """Get the latest version of a prompt family"""
        query = self.db.query(Prompt)
        if parent_id:
            query = query.filter(Prompt.parent_id == uuid.UUID(parent_id))
        return query.order_by(desc(Prompt.version)).first()
    
    def list_versions(self, parent_id: str) -> List[Prompt]:
        """List all versions of a prompt family"""
        return self.db.query(Prompt).filter(
            (Prompt.id == uuid.UUID(parent_id)) | (Prompt.parent_id == uuid.UUID(parent_id))
        ).order_by(Prompt.version).all()
    
    def to_prompt_state(self, prompt: Prompt) -> PromptState:
        """Convert database model to domain model"""
        return PromptState(
            system_prompt=prompt.system_prompt,
            output_prompt=prompt.output_prompt,
            id=str(prompt.id),
            parent_id=str(prompt.parent_id) if prompt.parent_id else None,
            version=prompt.version,
            metadata=prompt.metadata,
            created_at=prompt.created_at
        )
        # alembic/env.py
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# Add the src directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Import the SQLAlchemy models
from src.app.database.db import Base
from src.app.models.database_models import Prompt, Dataset, Experiment, MetricsRecord

# This is the Alembic Config object
config = context.config

# Get database URL from environment
db_url = os.getenv("DATABASE_URL", "postgresql://promptopt:devpassword@localhost:5432/promptopt")
config.set_main_option("sqlalchemy.url", db_url)

# Interpret the config file for Python logging
fileConfig(config.config_file_name)

# Set up target metadata
target_metadata = Base.metadata

# Other Alembic configuration
def run_migrations_offline():
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )
    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )
    with connectable.connect() as connection:
        context.configure(
            connection=connection, 
            target_metadata=target_metadata,
            compare_type=True,
        )
        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
    # docker-compose.yml
version: '3.8'

services:
  db:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=devpassword
      - POSTGRES_USER=promptopt
      - POSTGRES_DB=promptopt
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U promptopt"]
      interval: 5s
      timeout: 5s
      retries: 5

  api:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      db:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://promptopt:devpassword@db:5432/promptopt
      - ENV=development
      - VERTEX_PROJECT_ID=${VERTEX_PROJECT_ID}
      - VERTEX_LOCATION=${VERTEX_LOCATION}
      - PRIMARY_MODEL_NAME=${PRIMARY_MODEL_NAME}
      - OPTIMIZER_MODEL_NAME=${OPTIMIZER_MODEL_NAME}
      - GCS_BUCKET_NAME=${GCS_BUCKET_NAME}
    ports:
      - "8000:8000"
    volumes:
      - ./:/app
    command: >
      bash -c "alembic upgrade head && 
               uvicorn src.app.main:app --host 0.0.0.0 --port 8000 --reload"

  prefect:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      db:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://promptopt:devpassword@db:5432/promptopt
      - ENV=development
      - PREFECT_API_URL=${PREFECT_API_URL}
      - VERTEX_PROJECT_ID=${VERTEX_PROJECT_ID}
      - VERTEX_LOCATION=${VERTEX_LOCATION}
      - PRIMARY_MODEL_NAME=${PRIMARY_MODEL_NAME}
      - OPTIMIZER_MODEL_NAME=${OPTIMIZER_MODEL_NAME}
      - GCS_BUCKET_NAME=${GCS_BUCKET_NAME}
    ports:
      - "4200:4200"
    volumes:
      - ./:/app
    command: prefect server start --host 0.0.0.0 --port 4200

volumes:
  postgres_data:
  # Dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Set environment variables
ENV PYTHONPATH=/app

# Default command
CMD ["uvicorn", "src.app.main:app", "--host", "0.0.0.0", "--port", "8000"]
# requirements.txt
fastapi==0.100.0
uvicorn[standard]==0.23.2
pydantic==2.0.0
pydantic-settings==2.0.0
python-dotenv==1.0.0
prefect==2.13.0
google-cloud-aiplatform==1.36.0
evaluate==0.4.0
pandas==2.0.0
sqlalchemy==2.0.23
alembic==1.12.0
psycopg2==2.9.9  # PostgreSQL adapter
click==8.1.7
gcsfs==2023.6.0
joblib==1.3.0
# src/api/endpoints/experiments.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List
from uuid import UUID

from src.app.database.db import get_db
from src.app.repositories.experiment_repository import ExperimentRepository
from src.app.repositories.prompt_repository import PromptRepository
from src.app.repositories.dataset_repository import DatasetRepository
from src.api.models import ExperimentCreate, ExperimentResponse, ExperimentMetricsResponse
from src.flows.prompt_optimization_flow import start_optimization_flow

router = APIRouter(prefix="/experiments", tags=["Experiments"])

@router.post("/", response_model=ExperimentResponse)
async def create_experiment(experiment_data: ExperimentCreate, db: Session = Depends(get_db)):
    # Validate that prompt and dataset exist
    prompt_repo = PromptRepository(db)
    dataset_repo = DatasetRepository(db)
    
    prompt = prompt_repo.get_by_id(experiment_data.initial_prompt_id)
    if not prompt:
        raise HTTPException(status_code=404, detail="Prompt not found")
    
    dataset = dataset_repo.get_by_id(experiment_data.dataset_id)
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    # Create experiment
    experiment_repo = ExperimentRepository(db)
    experiment = experiment_repo.create(
        name=experiment_data.name,
        initial_prompt_id=experiment_data.initial_prompt_id,
        dataset_id=experiment_data.dataset_id,
        metrics=experiment_data.metrics,
        max_epochs=experiment_data.max_epochs,
        target_threshold=experiment_data.target_threshold
    )
    
    return ExperimentResponse.from_orm(experiment)

@router.post("/{experiment_id}/start")
async def start_experiment(experiment_id: UUID, db: Session = Depends(get_db)):
    experiment_repo = ExperimentRepository(db)
    experiment = experiment_repo.get_by_id(str(experiment_id))
    
    if not experiment:
        raise HTTPException(status_code=404, detail="Experiment not found")
    
    if experiment.status != "created":
        raise HTTPException(status_code=400, detail=f"Experiment is already {experiment.status}")
    
    # Update status
    experiment_repo.update_status(str(experiment_id), "queued")
    
    # Start the optimization flow
    flow_id = start_optimization_flow.delay(
        experiment_id=str(experiment_id)
    )
    
    return {"experiment_id": experiment_id, "status": "queued", "flow_id": flow_id}

    # src/cli.py
import click
import os
from alembic.config import Config
from alembic import command
import sys
from pathlib import Path
import psycopg2
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT

# Get base directory  
BASE_DIR = Path(__file__).resolve().parent.parent

@click.group()
def cli():
    """Command line interface for prompt optimizer database management"""
    pass

@cli.command()
def init_db():
    """Initialize the PostgreSQL database and run migrations"""
    # Parse database URL
    db_url = os.getenv("DATABASE_URL", "postgresql://promptopt:devpassword@localhost:5432/promptopt")
    parts = db_url.split("/")
    db_name = parts[-1]
    connection_str = "/".join(parts[:-1]) + "/postgres"
    
    try:
        # Connect to postgres database to create our database
        conn = psycopg2.connect(connection_str)
        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
        cursor = conn.cursor()
        
        # Check if database exists
        cursor.execute(f"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db_name}'")
        exists = cursor.fetchone()
        
        if not exists:
            click.echo(f"Creating database {db_name}")
            cursor.execute(f'CREATE DATABASE "{db_name}"')
        
        cursor.close()
        conn.close()
        
        # Run migrations
        alembic_cfg = Config(str(BASE_DIR / "alembic.ini"))
        command.upgrade(alembic_cfg, "head")
        click.echo("Database initialized and migrations applied")
        
    except Exception as e:
        click.echo(f"Error initializing database: {e}")
        sys.exit(1)

@cli.command()
@click.argument("message")
def create_migration(message):
    """Create a new migration"""
    alembic_cfg = Config(str(BASE_DIR / "alembic.ini"))
    command.revision(alembic_cfg, autogenerate=True, message=message)
    click.echo(f"Migration created with message: {message}")

@cli.command()
def upgrade_db():
    """Upgrade database to latest migration"""
    alembic_cfg = Config(str(BASE_DIR / "alembic.ini"))
    command.upgrade(alembic_cfg, "head")
    click.echo("Database upgraded to latest migration")

@cli.command()
@click.option("--tag", help="Tag name for the backup")
def backup_db(tag):
    """Backup PostgreSQL database"""
    db_url = os.getenv("DATABASE_URL", "postgresql://promptopt:devpassword@localhost:5432/promptopt")
    parts = db_url.split("/")
    db_name = parts[-1]
    user_pass = parts[2].split("@")[0]
    username = user_pass.split(":")[0]
    host_port = parts[2].split("@")[1]
    host = host_port.split(":")[0]
    
    # Create backups directory if it doesn't exist
    backup_dir = BASE_DIR / "backups"
    backup_dir.mkdir(exist_ok=True)
    
    # Create backup filename
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    tag_suffix = f"_{tag}" if tag else ""
    backup_file = backup_dir / f"{db_name}_{timestamp}{tag_suffix}.sql"
    
    # Run pg_dump command
    command = f"pg_dump -h {host} -U {username} -d {db_name} -f {backup_file}"
    click.echo(f"Running backup command: {command}")
    
    if os.system(command) == 0:
        click.echo(f"Database backed up to {backup_file}")
    else:
        click.echo("Database backup failed")

@cli.command()
@click.argument("backup_file")
def restore_db(backup_file):
    """Restore PostgreSQL database from backup"""
    db_url = os.getenv("DATABASE_URL", "postgresql://promptopt:devpassword@localhost:5432/promptopt")
    parts = db_url.split("/")
    db_name = parts[-1]
    user_pass = parts[2].split("@")[0]
    username = user_pass.split(":")[0]
    host_port = parts[2].split("@")[1]
    host = host_port.split(":")[0]
    
    backup_path = Path(backup_file)
    if not backup_path.exists():
        backup_path = BASE_DIR / "backups" / backup_file
        if not backup_path.exists():
            click.echo(f"Backup file not found: {backup_file}")
            return
    
    # Run psql command
    command = f"psql -h {host} -U {username} -d {db_name} -f {backup_path}"
    click.echo(f"Running restore command: {command}")
    
    if os.system(command) == 0:
        click.echo(f"Database restored from {backup_path}")
    else:
        click.echo("Database restore failed")

if __name__ == "__main__":
    cli()

    # .env.example
# Database
DATABASE_URL=postgresql://promptopt:devpassword@localhost:5432/promptopt
ENV=development

# Vertex AI
VERTEX_PROJECT_ID=your-gcp-project-id
VERTEX_LOCATION=us-central1
PRIMARY_MODEL_NAME=gemini-1.5-flash-001
OPTIMIZER_MODEL_NAME=gemini-1.5-pro-001

# Storage
GCS_BUCKET_NAME=your-gcs-bucket-for-artifacts

# Prefect
PREFECT_API_URL=http://127.0.0.1:4200/api
PREFECT_API_KEY=

#!/bin/bash
# deploy.sh - Deploy to production environment

# Set environment variables
export ENV=production
export DATABASE_URL=postgresql://promptopt:secretpassword@db.production.host:5432/promptopt_prod

# Pull latest code
git pull

# Install dependencies
pip install -r requirements.txt

# Run database migrations
python -m src.cli upgrade_db

# Run prefect deployment
prefect deployment build src.flows.prompt_optimization_flow:prompt_optimization_flow -n prompt-optimizer
prefect deployment apply prompt_optimization_flow-deployment.yaml

# Start the API server (using systemd in production)
sudo systemctl restart prompt-optimizer-api



   sudo apt-get update
   sudo apt-get install docker.io docker-compose

\   docker-compose up -d

   curl http://localhost:8000/health

      docker-compose exec api python -m src.cli create_migration "describe your changes"
   docker-compose exec api python -m src.cli upgrade_db
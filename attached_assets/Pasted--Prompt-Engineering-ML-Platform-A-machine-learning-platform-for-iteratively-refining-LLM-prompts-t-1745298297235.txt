# Prompt Engineering ML Platform

A machine learning platform for iteratively refining LLM prompts through automated optimization.

## Overview

This platform enables prompt engineers to systematically improve system prompts and output prompts through a machine-learning-style interface. It uses a **three-LLM architecture operating in an autonomous feedback loop**:

1. **Primary LLM**: Processes user inputs with the current `system_prompt` and `output_prompt`.
2. **Evaluation Engine**: Measures response quality against ground truth.
3. **Optimizer LLM**: Analyzes evaluation results (especially failures) and **autonomously proposes corrected versions** of the `system_prompt` and `output_prompt` to improve performance.

The platform implements a complete ML training loop for prompt engineering, where prompts are automatically tested and refined based on their measured performance, with metrics tracking, version history, and visualization.

## System Architecture

```
┌───────────────────┐     ┌────────────────────┐     ┌───────────────────┐
│                   │     │                    │     │                   │
│  PRIMARY LLM      │────▶│  EVALUATION        │────▶│  OPTIMIZER LLM    │
│  (Vertex AI)      │     │                    │     │  (Vertex AI)      │
│                   │     │                    │     │                   │
└───────────────────┘     └────────────────────┘     └───────────────────┘
        ▲                          │                          │
        │                          │                          │
        │                          ▼                          │
        │                 ┌────────────────────┐             │
        │                 │                    │             │
        │                 │  METRICS DISPLAY   │             │
        │                 │                    │             │
        └─────────────────│                    │◀────────────┘
                          │                    │
                          └────────────────────┘
```

## Key Features

- **ML-style Training Interface**: Configure epochs, batch sizes, and optimization parameters
- **Automated Prompt Refinement Loop**: The Optimizer LLM **autonomously suggests and tests** prompt improvements in a closed loop
- **Metrics Tracking**: Visualize performance improvements across training iterations
- **Version Control**: Track prompt evolution with full history and comparisons
- **Train/Validation Split**: Properly validate prompt improvements to prevent overfitting
- **Google Vertex AI Integration**: Leverages Google's powerful LLM models

## Project Structure

```
prompt-ml-platform/
├── app/
│   ├── static/
│   │   ├── css/
│   │   │   └── styles.css
│   │   ├── js/
│   │   │   ├── main.js
│   │   │   ├── training.js          # ML training loop UI logic
│   │   │   ├── visualization.js     # Charts and visualization
│   │   │   └── optimization.js       # Optimization logic
│   │   └── img/
│   ├── templates/
│   │   ├── index.html               # Main ML interface
│   │   ├── history.html             # Prompt version history
│   │   └── compare.html             # Side-by-side comparison
│   ├── __init__.py
│   ├── main.py                      # Flask routes
│   ├── llm_client.py                # Vertex AI interaction
│   ├── optimizer.py                 # Second LLM optimization logic
│   ├── evaluator.py                 # Metrics calculation
│   ├── data_module.py               # Dataset handling
│   └── experiment_tracker.py        # Tracking metrics/versions
│
├── data/
│   ├── train/                       # Training examples
│   └── validation/                  # Validation examples
│
├── prompts/
│   ├── system/                      # System prompts versions
│   ├── output/                      # Output prompts versions
│   └── optimizer/                   # Optimizer LLM prompts
│       └── default_optimizer.txt    # Default instructions for optimizer
│
├── experiments/                     # Saved experiment results
│   └── metrics/                     # Metrics over time
│
├── config.yaml                      # Configuration
├── requirements.txt
└── README.md
```

## Core Workflow

### 1. Initial Setup
- Configure Vertex AI credentials
- Define initial system and output prompts
- Upload training/validation data (CSV with `user_input`/`ground_truth` pairs)
- Configure the initial instructions for the Optimizer LLM

### 2. Training Loop (Autonomous Refinement)
- The Primary LLM processes training examples with the current `system_prompt` and `output_prompt`
- The Evaluation Engine calculates performance metrics by comparing responses to the `ground_truth_output`
- The Optimizer LLM receives performance data (focusing on failures) and **autonomously proposes corrected versions** of the `system_prompt` and `output_prompt`
- **Crucially, these new prompts are then automatically tested**: The Primary LLM reruns queries (typically on a validation set) using the *corrected* prompts
- The Evaluation Engine assesses if the new prompts yield outputs closer to the `ground_truth_output`
- If performance improves (based on validation metrics), the corrected prompts are accepted and become the new baseline for the next iteration
- This feedback loop repeats for the specified number of epochs or until early stopping criteria are met

### 3. Analysis & Export
- Review metrics and prompt evolution over the training iterations
- Compare different prompt versions
- Export final optimized prompts

## Machine Learning Interface

The UI follows standard ML training conventions:

**Prompt Configuration Panel:**
- Input areas for system_prompt and output_prompt
- Version history dropdown

**Training Data Management:**
- CSV upload for input/output pairs
- Train/validation split options
- Dataset statistics

**Training Control Panel:**
- Start/stop training
- Epochs and batch size settings
- Early stopping options

**Optimizer LLM Configuration:**
- Configuration for optimizer prompt
- Strategy selection options

**Results Dashboard:**
- Metrics visualization
- Before/after examples table
- Prompt evolution display

**Logs Panel:**
- Real-time training progress
- Optimizer reasoning logs

## Installation

### Prerequisites
- Python 3.8+
- Google Cloud account with Vertex AI access
- Application Default Credentials configured

### Setup
1. Clone this repository
```
git clone https://github.com/yourusername/prompt-ml-platform.git
cd prompt-ml-platform
```

2. Install dependencies
```
pip install -r requirements.txt
```

3. Configure Vertex AI credentials
```
gcloud auth application-default login
```

4. Update configuration
```
# Edit config.yaml with your project details
project_id: "your-gcp-project-id"
location: "us-central1"
primary_model: "gemini-1.5-pro"
optimizer_model: "gemini-1.5-pro"
```

5. Run the application
```
python -m app.main
```

## Usage Guide

### Preparing Your Dataset
Create a CSV file with the following columns:
- `user_input`: The input text to be sent to the LLM
- `ground_truth_output`: The expected output

Example:
```
user_input,ground_truth_output
"Translate to French: Hello world","Bonjour le monde"
"Translate to French: Good morning","Bonjour"
```

### Initial Prompts
Start with a basic system prompt and output prompt:

System Prompt Example:
```
You are a helpful, precise assistant that translates English to French.
```

Output Prompt Example:
```
Translate the above text to French. Return only the translation with no additional text.
```

### Optimizer Prompt
The default optimizer prompt instructs the second LLM how to analyze and improve prompts:

```
You are a prompt engineering expert. Your task is to analyze the performance of a language model on specific examples and suggest improvements to the prompts.

You will receive:
1. The current system prompt
2. The current output prompt
3. A set of examples showing:
   - User input
   - Expected output (ground truth)
   - Actual model response
   - Score (1 for match, 0 for mismatch)
4. Overall metrics

Analyze the examples where the model fails or performs poorly. Look for patterns in the errors. Then suggest revised versions of:
1. The system prompt
2. The output prompt

Your response MUST follow this JSON format:
{
  "system_prompt": "your revised system prompt here",
  "output_prompt": "your revised output prompt here",
  "reasoning": "detailed explanation of your changes and how they address the failures"
}

Focus on making the prompts more precise, adding constraints where needed, and clarifying instructions to guide the model toward the correct output format and content.
```

### Running a Training Session
1. Upload your dataset or use the sample data
2. Enter your initial system prompt and output prompt
3. Configure training parameters (epochs, batch size)
4. Click "Start Training" to initiate the **autonomous optimization loop**
5. Monitor progress in real-time as the system iterates and refines prompts
6. Review final optimized prompts when training completes

## Technical Details

### API Integration
The platform uses the Google Cloud Vertex AI API to access LLMs:
- `google-cloud-aiplatform` Python library
- Vertex AI GenerativeModels for text generation
- Temperature 0 for reproducible results during training

### Evaluation Metrics
Default metrics include:
- Exact match (1 for perfect match, 0 otherwise)
- Optional: BLEU score for translation tasks
- Optional: Semantic similarity for paraphrase-type tasks

Custom evaluators can be implemented in `evaluator.py`.

### Optimization Strategies
The platform includes multiple optimization approaches:
1. **Full Rewrite**: Optimizer LLM completely rewrites prompts
2. **Targeted Edit**: Optimizer suggests specific changes to problematic sections
3. **Example Addition**: Dynamically adds few-shot examples to the prompts

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Inspired by research in "Learning to Generate Better Than Your LLM" (DSPy)
- Architecture influenced by standard ML training pipelines
- Special thanks to the Vertex AI team for making powerful LLMs accessible 
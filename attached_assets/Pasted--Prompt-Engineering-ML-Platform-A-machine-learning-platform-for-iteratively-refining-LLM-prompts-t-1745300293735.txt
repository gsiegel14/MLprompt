# Prompt Engineering ML Platform

A machine learning platform for iteratively refining LLM prompts through automated optimization.

## Overview

This platform enables prompt engineers to systematically improve system prompts and output prompts through a machine-learning-style interface. It uses a **three-LLM architecture operating in an autonomous feedback loop**:

1. **Primary LLM**: Processes user inputs with the current `system_prompt` and `output_prompt`.
2. **Evaluation Engine**: Measures response quality against ground truth.
3. **Optimizer LLM**: Analyzes evaluation results (especially failures) and **autonomously proposes corrected versions** of the `system_prompt` and `output_prompt` to improve performance.

The platform implements a complete ML training loop for prompt engineering, where prompts are automatically tested and refined based on their measured performance, with metrics tracking, version history, and visualization.

## System Architecture

```
┌───────────────────┐     ┌────────────────────┐     ┌───────────────────┐
│                   │     │                    │     │                   │
│  PRIMARY LLM      │────▶│  EVALUATION        │────▶│  OPTIMIZER LLM    │
│  (Vertex AI)      │     │                    │     │  (Vertex AI)      │
│                   │     │                    │     │                   │
└───────────────────┘     └────────────────────┘     └───────────────────┘
        ▲                          │                          │
        │                          │                          │
        │                          ▼                          │
        │                 ┌────────────────────┐             │
        │                 │                    │             │
        │                 │  METRICS DISPLAY   │             │
        │                 │                    │             │
        └─────────────────│                    │◀────────────┘
                          │                    │
                          └────────────────────┘
```

## Key Features

- **ML-style Training Interface**: Configure epochs, batch sizes, and optimization parameters
- **Automated Prompt Refinement Loop**: The Optimizer LLM **autonomously suggests and tests** prompt improvements in a closed loop
- **Metrics Tracking**: Visualize performance improvements across training iterations
- **Version Control**: Track prompt evolution with full history and comparisons
- **Train/Validation Split**: Properly validate prompt improvements to prevent overfitting
- **Google Vertex AI Integration**: Leverages Google's powerful LLM models

## Project Structure

```
prompt-ml-platform/
├── app/
│   ├── static/
│   │   ├── css/
│   │   │   └── styles.css
│   │   ├── js/
│   │   │   ├── main.js
│   │   │   ├── training.js          # ML training loop UI logic
│   │   │   ├── visualization.js     # Charts and visualization
│   │   │   └── optimization.js       # Optimization logic
│   │   └── img/
│   ├── templates/
│   │   ├── index.html               # Main ML interface
│   │   ├── history.html             # Prompt version history
│   │   └── compare.html             # Side-by-side comparison
│   ├── __init__.py
│   ├── main.py                      # Flask routes
│   ├── llm_client.py                # Vertex AI interaction
│   ├── optimizer.py                 # Second LLM optimization logic
│   ├── evaluator.py                 # Metrics calculation
│   ├── data_module.py               # Dataset handling
│   └── experiment_tracker.py        # Tracking metrics/versions
│
├── data/
│   ├── train/                       # Training examples
│   └── validation/                  # Validation examples
│
├── prompts/
│   ├── system/                      # System prompts versions
│   ├── output/                      # Output prompts versions
│   └── optimizer/                   # Optimizer LLM prompts
│       └── default_optimizer.txt    # Default instructions for optimizer
│
├── experiments/                     # Saved experiment results
│   └── metrics/                     # Metrics over time
│
├── config.yaml                      # Configuration
├── requirements.txt
└── README.md
```

## Core Workflow

### 1. Initial Setup
- Configure Vertex AI credentials
- Define initial system and output prompts
- Upload training/validation data (CSV with `user_input`/`ground_truth` pairs)
- Configure the initial instructions for the Optimizer LLM

### 2. Training Loop (Autonomous Refinement)
- The Primary LLM processes training examples with the current `system_prompt` and `output_prompt`
- The Evaluation Engine calculates performance metrics by comparing responses to the `ground_truth_output`
- The Optimizer LLM receives performance data (focusing on failures) and **autonomously proposes corrected versions** of the `system_prompt` and `output_prompt`
- **Crucially, these new prompts are then automatically tested**: The Primary LLM reruns queries (typically on a validation set) using the *corrected* prompts
- The Evaluation Engine assesses if the new prompts yield outputs closer to the `ground_truth_output`
- If performance improves (based on validation metrics), the corrected prompts are accepted and become the new baseline for the next iteration
- This feedback loop repeats for the specified number of epochs or until early stopping criteria are met

### 3. Analysis & Export
- Review metrics and prompt evolution over the training iterations
- Compare different prompt versions
- Export final optimized prompts

## Machine Learning Interface

The UI follows standard ML training conventions:

**Prompt Configuration Panel:**
- Input areas for system_prompt and output_prompt
- Version history dropdown

**Training Data Management:**
- CSV upload for input/output pairs
- Train/validation split options
- Dataset statistics

**Training Control Panel:**
- Start/stop training
- Epochs and batch size settings
- Early stopping options

**Optimizer LLM Configuration:**
- Configuration for optimizer prompt
- Strategy selection options

**Results Dashboard:**
- Metrics visualization
- Before/after examples table
- Prompt evolution display

**Logs Panel:**
- Real-time training progress
- Optimizer reasoning logs

## Installation

### Prerequisites
- Python 3.8+
- Google Cloud account with Vertex AI access
- Application Default Credentials configured

### Setup
1. Clone this repository
```
git clone https://github.com/yourusername/prompt-ml-platform.git
cd prompt-ml-platform
```

2. Install dependencies
```
pip install -r requirements.txt
```

3. Configure Vertex AI credentials
```
gcloud auth application-default login
```

4. Update configuration
```
# Edit config.yaml with your project details
project_id: "your-gcp-project-id"
location: "us-central1"
primary_model: "gemini-1.5-pro"
optimizer_model: "gemini-1.5-pro"
```

5. Run the application
```
python -m app.main
```

## Usage Guide

### Preparing Your Dataset
Create a CSV file with the following columns:
- `user_input`: The input text to be sent to the LLM
- `ground_truth_output`: The expected output

Example:
```
user_input,ground_truth_output
"Translate to French: Hello world","Bonjour le monde"
"Translate to French: Good morning","Bonjour"
```

### Initial Prompts
Start with a basic system prompt and output prompt:

System Prompt Example:
```
You are a helpful, precise assistant that translates English to French.
```

Output Prompt Example:
```
Translate the above text to French. Return only the translation with no additional text.
```

### Optimizer Prompt
The default optimizer prompt instructs the second LLM how to analyze and improve prompts:

```
You are a prompt engineering expert. Your task is to analyze the performance of a language model on specific examples and suggest improvements to the prompts.

You will receive:
1. The current system prompt
2. The current output prompt
3. A set of examples showing:
   - User input
   - Expected output (ground truth)
   - Actual model response
   - Score (1 for match, 0 for mismatch)
4. Overall metrics

Analyze the examples where the model fails or performs poorly. Look for patterns in the errors. Then suggest revised versions of:
1. The system prompt
2. The output prompt

Your response MUST follow this JSON format:
{
  "system_prompt": "your revised system prompt here",
  "output_prompt": "your revised output prompt here",
  "reasoning": "detailed explanation of your changes and how they address the failures"
}

Focus on making the prompts more precise, adding constraints where needed, and clarifying instructions to guide the model toward the correct output format and content.
```

### Running a Training Session
1. Upload your dataset or use the sample data
2. Enter your initial system prompt and output prompt
3. Configure training parameters (epochs, batch size)
4. Click "Start Training" to initiate the **autonomous optimization loop**
5. Monitor progress in real-time as the system iterates and refines prompts
6. Review final optimized prompts when training completes

## Technical Details

### API Integration
The platform uses the Google Cloud Vertex AI API to access LLMs:
- `google-cloud-aiplatform` Python library
- Vertex AI GenerativeModels for text generation
- Temperature 0 for reproducible results during training

### Evaluation Metrics
Default metrics include:
- Exact match (1 for perfect match, 0 otherwise)
- Optional: BLEU score for translation tasks
- Optional: Semantic similarity for paraphrase-type tasks

Custom evaluators can be implemented in `evaluator.py`.

### Optimization Strategies
The platform includes multiple optimization approaches:
1. **Full Rewrite**: Optimizer LLM completely rewrites prompts
2. **Targeted Edit**: Optimizer suggests specific changes to problematic sections
3. **Example Addition**: Dynamically adds few-shot examples to the prompts

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Inspired by research in "Learning to Generate Better Than Your LLM" (DSPy)
- Architecture influenced by standard ML training pipelines
- Special thanks to the Vertex AI team for making powerful LLMs accessible 

# Prompt Engineering ML Platform: Implementation Deep Dive

This document provides a deeper look into the implementation details of the autonomous prompt engineering workflow.

## 1. Detailed Data Flow (Single Epoch)

The following diagram illustrates the data flow and component interactions within a single training epoch:

```mermaid
graph TD
    A[Start Epoch] --> B(Load Training Batch);
    B --> C{Run Primary LLM};
    C -- PromptState (vN), Examples --> D[Generate Responses];
    D -- Responses --> E{Evaluate Responses};
    E -- Ground Truth --> F[Calculate Metrics (Accuracy, etc.)];
    F -- Results & Metrics --> G{Log Training Metrics};
    G --> H(Prepare Context for Optimizer);
    H -- Current Prompts(vN), Failed Examples, Metrics, Optimizer Instructions --> I{Call Optimizer LLM};
    I --> J[Parse Optimizer Response];
    J -- Proposed Prompts (vN+1), Reasoning --> K{Log Optimizer Action};
    K --> L(Load Validation Batch);
    L --> M{Run Primary LLM};
    M -- Proposed Prompts(vN+1), Val Examples --> N[Generate Validation Responses];
    N -- Val Responses --> O{Evaluate Validation Responses};
    O -- Val Ground Truth --> P[Calculate Validation Metrics];
    P -- New Val Score --> Q{Compare with Best Score};
    Q -- Improved? --> R(Update PromptState to vN+1);
    R --> S[Save Checkpoint (Prompts vN+1)];
    Q -- Not Improved? --> T(Keep PromptState vN);
    S --> U[End Epoch];
    T --> U;

    style R fill:#d4edda,stroke:#c3e6cb
    style T fill:#f8d7da,stroke:#f5c6cb
```

## 2. Deeper Dive into Key Modules

### a. `llm_client.py` (Vertex AI Interaction)

**Purpose:** Handles all communication with the Google Cloud Vertex AI API for both the primary task LLM and the optimizer LLM.

**Class Structure:**

```python
from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel, Part
import json

class VertexAIClient:
    def __init__(self, project_id: str, location: str):
        aiplatform.init(project=project_id, location=location)
        self.models = {} # Cache loaded models

    def _load_model(self, model_name: str) -> GenerativeModel:
        if model_name not in self.models:
            self.models[model_name] = GenerativeModel(model_name)
        return self.models[model_name]

    def generate_response(self, model_name: str, system_prompt: str | None, user_content: str) -> str:
        """Generates response using the primary task model."""
        model = self._load_model(model_name)
        prompt_parts = []
        # Actual implementation needs careful handling of system prompts for Vertex AI
        # This might involve specific structuring or using multi-turn context.
        # Assuming user_content includes necessary context for now.
        if system_prompt:
             # Placeholder - needs correct Gemini API usage for system prompts
             # Example: Prepend system instructions if API supports it directly
             # Or structure messages appropriately if using chat interface
             pass

        prompt_parts.append(Part.from_text(user_content))

        try:
            # Use temperature=0.0 for deterministic primary LLM responses during eval
            response = model.generate_content(
                prompt_parts,
                generation_config={"temperature": 0.0}
            )
            # Add safety checks on response.candidates if necessary
            return response.text
        except Exception as e:
            print(f"Error generating response: {e}")
            # Log error appropriately
            return "[ERROR]"

    def suggest_prompt_optimizations(self, model_name: str, optimizer_instructions: str, context: str) -> dict:
        """Calls the optimizer LLM to get new prompts."""
        model = self._load_model(model_name)
        # Structure prompt for optimizer LLM
        prompt_parts = [
            # Part.from_text(optimizer_instructions), # How optimizer should behave (might be part of context or specific API param)
            Part.from_text(context) # The data (prompts, examples, metrics) + instructions
        ]
        try:
            # Ensure optimizer provides JSON
            response = model.generate_content(
                prompt_parts,
                generation_config={"response_mime_type": "application/json"}
            )
            # Add safety checks
            return json.loads(response.text)
        except (json.JSONDecodeError, Exception) as e:
            print(f"Error calling optimizer or parsing JSON: {e}")
            # Log error
            return {"error": str(e), "raw_response": getattr(response, 'text', None)}
```

**Key Considerations:**
-   Correctly format requests for specific Vertex AI models (Gemini requires specific handling of system prompts/instructions).
-   Implement robust error handling, retries (e.g., using `tenacity`), and potentially rate limiting.
-   Reliably parse the JSON output from the optimizer LLM, handling malformed responses gracefully.
-   Manage API credentials securely (using Application Default Credentials is recommended).
-   Cache loaded models to avoid re-initialization costs.

### b. `optimizer.py` (Orchestrating the Optimization)

**Purpose:** Formats data for the optimizer LLM, calls the `VertexAIClient` to get suggestions, and parses the results.

**Class Structure:**

```python
import json
from .llm_client import VertexAIClient # Assuming llm_client is in the same directory

class PromptOptimizer:
    def __init__(self, llm_client: VertexAIClient, optimizer_model_name: str, optimizer_instructions_template: str):
        self.llm_client = llm_client
        self.optimizer_model_name = optimizer_model_name
        # Template might include placeholders for prompts, examples etc.
        self.optimizer_instructions_template = optimizer_instructions_template

    def _format_context_for_optimizer(self, current_system_prompt, current_output_prompt, examples, metrics) -> str:
        """Formats the data into a single string prompt for the optimizer LLM."""
        # Select and format k worst examples or a representative sample
        # Ensure formatting is clear and easily parsable by the LLM
        formatted_examples = []
        for ex in examples: # Assume examples is a list of dicts
            formatted_examples.append(
                f"- Input: {ex['user_input']}\n"
                f"  Ground Truth: {ex['ground_truth_output']}\n"
                f"  Actual Output: {ex['model_response']}\n"
                f"  Score: {ex['score']}\n" # Or 'Status: FAILED/PASSED'
            )
        examples_str = "\n".join(formatted_examples)

        # Use the template, injecting the dynamic data
        # This allows the core instructions to be managed separately
        context = self.optimizer_instructions_template.format(
            current_system_prompt=current_system_prompt,
            current_output_prompt=current_output_prompt,
            metrics_json=json.dumps(metrics, indent=2),
            examples_str=examples_str
        )
        return context

    def propose_new_prompts(self, current_system_prompt, current_output_prompt, examples, metrics) -> tuple[str | None, str | None, str | None]:
        """Gets refined prompts from the optimizer LLM."""
        context_str = self._format_context_for_optimizer(current_system_prompt, current_output_prompt, examples, metrics)

        # Pass the fully formatted context (which includes instructions) to the client
        response_data = self.llm_client.suggest_prompt_optimizations(
            model_name=self.optimizer_model_name,
            optimizer_instructions="", # Instructions are now part of context_str
            context=context_str
        )

        # Validate response structure
        if "error" in response_data or not isinstance(response_data, dict) or \
           "system_prompt" not in response_data or "output_prompt" not in response_data:
            print(f"Optimizer failed or returned invalid format: {response_data}")
            return None, None, None # Indicate failure

        reasoning = response_data.get("reasoning", "No reasoning provided.")
        return response_data["system_prompt"], response_data["output_prompt"], reasoning
```

**Key Considerations:**
-   Develop effective strategies for selecting and formatting examples to send to the optimizer (e.g., worst `k` examples, examples exhibiting specific error patterns, diverse sampling).
-   Craft clear and robust instructions (`optimizer_instructions_template`) guiding the optimizer LLM on its task and required JSON output format.
-   Handle failures or malformed JSON responses from the optimizer LLM gracefully.

### c. `trainer.py` (The Core Training Loop)

**Purpose:** Manages the overall training process, orchestrating the calls to other modules, tracking state, and implementing the autonomous refinement loop.

**Class Structure:**

```python
# Assuming imports for data_module, llm_client, optimizer, evaluator, tracker

class Trainer:
    def __init__(self, config, data_module, llm_client, optimizer, evaluator, tracker):
        self.config = config
        self.data_module = data_module
        self.llm_client = llm_client
        self.optimizer = optimizer
        self.evaluator = evaluator
        self.tracker = tracker
        # State
        self.current_epoch = 0
        self.best_val_score = -float('inf')
        self.epochs_no_improve = 0
        self.current_system_prompt = ""
        self.current_output_prompt = ""

    def _run_forward_pass(self, data, system_prompt, output_prompt, model_name):
        """Runs the primary LLM on a dataset."""
        results = []
        # Consider parallelizing this loop using asyncio or ThreadPoolExecutor
        for example in data:
            # Combine input + output prompt logic might need refinement
            user_content = example['user_input'] # System prompt handled by client
            if output_prompt:
                 user_content += "\n\n" + output_prompt # Append output instructions

            response = self.llm_client.generate_response(
                model_name=model_name,
                system_prompt=system_prompt,
                user_content=user_content
            )
            score = self.evaluator.score(response, example['ground_truth_output'])
            results.append({**example, "model_response": response, "score": score})
        return results

    def train(self):
        train_data, val_data = self.data_module.get_train_val_split()
        self.current_system_prompt = self.data_module.load_initial_prompt('system')
        self.current_output_prompt = self.data_module.load_initial_prompt('output')
        optimizer_instructions = self.data_module.load_optimizer_instructions()
        # Re-initialize optimizer with loaded instructions if template based
        # self.optimizer = PromptOptimizer(..., optimizer_instructions_template=optimizer_instructions)

        self.tracker.start_run(config=self.config) # Log config
        self.tracker.log_prompt("initial_system_prompt", self.current_system_prompt)
        self.tracker.log_prompt("initial_output_prompt", self.current_output_prompt)

        max_epochs = self.config['training']['max_epochs']
        patience = self.config['training']['early_stopping_patience']
        stop_metric = self.config['training']['early_stopping_metric']
        primary_model = self.config['vertexai']['primary_model']
        examples_k = self.config['optimizer']['examples_k']

        for epoch in range(max_epochs):
            self.current_epoch = epoch
            print(f"\n--- Epoch {epoch+1}/{max_epochs} ---")
            self.tracker.log_metric("epoch", epoch + 1)

            # 1. Train Forward Pass
            print("Running Train Forward Pass...")
            train_results = self._run_forward_pass(train_data, self.current_system_prompt, self.current_output_prompt, primary_model)
            train_metrics = self.evaluator.aggregate_metrics(train_results)
            self.tracker.log_metrics(train_metrics, prefix="train_", step=epoch)
            print(f"Train Metrics: {train_metrics}")

            # 2. Optimizer Step
            print("Calling Optimizer LLM...")
            optimizer_examples = [r for r in train_results if r['score'] < 1.0][:examples_k]
            if not optimizer_examples and len(train_results) > 0: # Handle case with no failures
                 optimizer_examples = train_results[:examples_k] # Send some examples anyway

            new_sys_prompt, new_out_prompt, reasoning = self.optimizer.propose_new_prompts(
                self.current_system_prompt, self.current_output_prompt, optimizer_examples, train_metrics
            )

            if new_sys_prompt is None: # Optimizer failed
                print("Optimizer failed to propose new prompts. Stopping run.")
                # Potentially try again, or stop the run
                break

            self.tracker.log_text("optimizer_reasoning", reasoning, step=epoch)
            print(f"Optimizer Reasoning (Epoch {epoch+1}): {reasoning[:100]}...")

            # 3. Validation Step (using NEW prompts)
            print("Running Validation Forward Pass (with proposed prompts)...")
            val_results = self._run_forward_pass(val_data, new_sys_prompt, new_out_prompt, primary_model)
            val_metrics = self.evaluator.aggregate_metrics(val_results)
            self.tracker.log_metrics(val_metrics, prefix="val_", step=epoch)
            print(f"Validation Metrics (Proposed Prompts): {val_metrics}")

            # 4. Acceptance & Early Stopping
            current_val_score = val_metrics.get(stop_metric, -float('inf'))
            if current_val_score > self.best_val_score:
                print(f"Validation score improved ({self.best_val_score:.4f} -> {current_val_score:.4f}). Accepting new prompts.")
                self.best_val_score = current_val_score
                self.current_system_prompt = new_sys_prompt
                self.current_output_prompt = new_out_prompt
                self.epochs_no_improve = 0
                # Log accepted prompts and save checkpoint
                self.tracker.log_prompt("accepted_system_prompt", self.current_system_prompt, step=epoch)
                self.tracker.log_prompt("accepted_output_prompt", self.current_output_prompt, step=epoch)
                # Consider saving checkpoint here (prompts, optimizer state, etc.)
            else:
                print(f"Validation score did not improve ({self.best_val_score:.4f} >= {current_val_score:.4f}). Rejecting proposed prompts.")
                self.epochs_no_improve += 1

            self.tracker.log_metric("best_val_score", self.best_val_score, step=epoch)
            if self.epochs_no_improve >= patience:
                print(f"Early stopping triggered after {epoch+1} epochs.")
                break

        self.tracker.end_run() # Finalize experiment tracking
        print("\nTraining finished.")
        print(f"Best Validation Score ({stop_metric}): {self.best_val_score:.4f}")
        print("Final System Prompt:", self.current_system_prompt)
        print("Final Output Prompt:", self.current_output_prompt)
```

**Key Considerations:**
-   Manage the state correctly (current prompts, best validation score, epochs without improvement).
-   Implement the core loop logic accurately, ensuring the validation step uses the *proposed* prompts before acceptance.
-   Integrate seamlessly with an experiment tracking library (like MLflow or WandB via `self.tracker`) to log parameters, metrics, prompts, and reasoning.
-   Implement early stopping based on validation metric performance.
-   Consider parallelizing the `_run_forward_pass` method using `asyncio` or `concurrent.futures` for significant speedups.
-   Structure configuration (`self.config`) clearly (e.g., loaded from `config.yaml`).

## 3. Advanced Considerations

-   **Asynchronous Operations:** The forward passes involving multiple LLM calls are IO-bound. Use `asyncio` with an async Vertex AI client (if available) or `concurrent.futures.ThreadPoolExecutor` to parallelize these calls and drastically reduce epoch times. The backend web server (Flask) should ideally be run with an async server like `uvicorn` or use background task queues (Celery, RQ) to handle the long-running `trainer.train()` process without blocking HTTP requests.
-   **State Management & Scalability:** If supporting multiple concurrent training jobs, manage state externally (e.g., Redis, database) to track job status, prompts per job, and progress. For large datasets/experiments, use database-backed storage for results and tracking.
-   **Error Handling & Resilience:** Implement comprehensive error handling for API calls, data parsing, and optimizer failures. Include fallback strategies or retry mechanisms.
-   **Testing Strategy:**
    -   *Unit Tests:* For `evaluator`, `data_module` functions, prompt formatting logic.
    -   *Integration Tests:* For `llm_client` (potentially using mocks/stubs or a dedicated test project), testing the interaction between `trainer` and `optimizer`.
    -   *End-to-End Tests:* Use a minimal dataset and mock LLM responses to verify the entire training loop logic.
-   **Configuration Management:** Use a clear configuration system (like Hydra or simple YAML files) to manage all parameters (model names, training settings, optimizer details, paths).

This deeper dive provides a more concrete blueprint for building the autonomous prompt optimization workflow. 
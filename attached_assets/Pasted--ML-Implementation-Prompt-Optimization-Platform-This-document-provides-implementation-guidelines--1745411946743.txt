# ML Implementation: Prompt Optimization Platform

This document provides implementation guidelines for building a prompt optimization platform with a 5-step API workflow, orchestrated by Prefect 2.0, and exposed via a FastAPI backend.

## Project Overview

The platform enables iterative refinement of LLM prompts through an ML-driven optimization loop:

1. **Primary LLM Inference** → Generate baseline responses with current prompts
2. **Hugging Face Evaluation** → Compute baseline metrics
3. **Optimizer LLM** → Generate refined prompts based on performance data
4. **Refined LLM Inference** → Run inference with optimized prompts
5. **Second Evaluation** → Compare metrics and decide whether to loop

The system leverages Vertex AI (Gemini), Hugging Face Evaluate for metrics, and Prefect for workflow orchestration.

## Project Structure

```
prompt_optimizer_platform/
├── .env.example             # Environment variables template
├── Dockerfile               # For FastAPI service
├── docker-compose.yml       # For local development
├── README.md                # Project overview
├── requirements.txt         # Python dependencies
├── config/                  # Configuration files
├── data/
│   ├── raw/                 # Original datasets
│   └── processed/           # Cleaned/split datasets
├── docs/                    # Project documentation
├── notebooks/               # Jupyter notebooks for exploration
├── prompts/                 # Default prompt templates
│   ├── initial_system.txt   # System prompt template
│   └── initial_output.txt   # Output instruction template
├── scripts/                 # Utility scripts
├── src/
│   ├── api/                 # FastAPI endpoints
│   │   ├── endpoints/       # API route implementations
│   │   ├── models.py        # Pydantic request/response models
│   │   └── routers.py       # API router aggregation
│   ├── app/                 # Core application logic
│   │   ├── clients/         # External service clients
│   │   ├── config.py        # Settings management
│   │   ├── dependencies.py  # FastAPI dependencies
│   │   ├── main.py          # FastAPI app entry point
│   │   ├── models/          # Internal data structures
│   │   ├── services/        # Business logic services
│   │   └── utils/           # Common utilities
│   └── flows/               # Prefect flows and tasks
│       ├── tasks/           # Reusable Prefect tasks
│       └── prompt_optimization_flow.py  # Main 5-step flow
└── tests/                   # Unit and integration tests
```

## Setup Instructions

1. **Create Environment & Install Dependencies**

```bash
# Create a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

2. **Configure Environment Variables**

```bash
# Copy the example file and edit with your values
cp .env.example .env
# Edit .env with your credentials and settings
```

3. **Start the Prefect Server** (optional for local development)

```bash
prefect server start
```

4. **Run the FastAPI Server**

```bash
uvicorn src.app.main:app --reload
```

## Core Components Implementation

### 1. Configuration Setup

Create `.env.example` with the required variables:

```dotenv
VERTEX_PROJECT_ID="your-gcp-project-id"
VERTEX_LOCATION="us-central1"
PRIMARY_MODEL_NAME="gemini-1.5-flash-001"
OPTIMIZER_MODEL_NAME="gemini-1.5-pro-001"
WANDB_API_KEY="your-wandb-key"  # Optional
GCS_BUCKET_NAME="your-gcs-bucket-for-artifacts"
PREFECT_API_URL="http://127.0.0.1:4200/api"  # For local server
PREFECT_API_KEY=""  # If using Prefect Cloud
```

Implement the Pydantic settings class:

```python
# src/app/config.py
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    VERTEX_PROJECT_ID: str
    VERTEX_LOCATION: str
    PRIMARY_MODEL_NAME: str
    OPTIMIZER_MODEL_NAME: str
    WANDB_API_KEY: Optional[str] = None
    GCS_BUCKET_NAME: str
    PREFECT_API_URL: Optional[str] = None
    PREFECT_API_KEY: Optional[str] = None

    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'

settings = Settings()
```

### 2. Core Data Models

Implement the `PromptState` class to manage prompt versions:

```python
# src/app/models/prompt_state.py
from pydantic import BaseModel
import json
from typing import Optional

class PromptState(BaseModel):
    system_prompt: str
    output_prompt: str
    version: int = 1
    
    def dict(self):
        return {
            "system_prompt": self.system_prompt,
            "output_prompt": self.output_prompt,
            "version": self.version
        }
    
    @classmethod
    def load(cls, path: str):
        # Implementation for loading from file/GCS
        pass
        
    def save(self, path: str):
        # Implementation for saving to file/GCS
        pass
```

### 3. External Service Clients

Implement client stubs for Vertex AI and Hugging Face:

```python
# src/app/clients/vertex_client.py
class VertexAIClient:
    def __init__(self, project_id: str, location: str):
        self.project_id = project_id
        self.location = location
        # Initialize client
        
    def batch_predict(self, df, prompt_state, model_name=None):
        """Run inference on dataset with given prompt state"""
        # Implementation
        
    def generate_response(self, model_name, user_content, temperature=0.7, response_mime_type=None):
        """Generate a single response from the LLM"""
        # Implementation
```

```python
# src/app/clients/hf_evaluator.py
class EvaluatorService:
    def __init__(self):
        # Initialize evaluator
        pass
        
    def evaluate(self, predictions, references, metrics=None):
        """Evaluate predictions against references using specified metrics"""
        # Implementation using evaluate library
        return {"exact_match_score": 0.75}  # Example return
```

### 4. Prefect Flow Implementation

Create the main optimization flow:

```python
# src/flows/prompt_optimization_flow.py
from prefect import flow, task, get_run_logger
from prefect.artifacts import create_artifact
import pandas as pd
import json

@task(name="load-state", retries=2)
def load_state(system_prompt_path, output_prompt_path, dataset_path, state_path=None):
    """Load or initialize PromptState & training data."""
    # Implementation
    
@task(name="vertex-primary-inference", retries=3)
def vertex_primary_inference(state_dict, dataset_dict, vertex_project_id, vertex_location, model_name):
    """Run primary inference with LLM"""
    # Implementation
    
# Define other tasks...

@flow(name="prompt-optimization-flow")
def prompt_optimization_flow(
    vertex_project_id: str,
    vertex_location: str,
    primary_model_name: str,
    optimizer_model_name: str,
    dataset_path: str,
    system_prompt_path: str,
    output_prompt_path: str,
    target_metric: str = "exact_match_score",
    target_threshold: float = 0.90,
    patience: int = 3,
    max_iterations: int = 10,
):
    """
    Main optimization flow that iteratively improves prompts using Vertex AI & HF Evaluate.
    """
    logger = get_run_logger()
    logger.info(f"Starting prompt optimization flow with target {target_metric} >= {target_threshold}")
    
    # Implement flow logic with task calls and iteration
```

### 5. API Implementation

Set up the basic FastAPI application:

```python
# src/app/main.py
from fastapi import FastAPI
from src.api.routers import api_router

app = FastAPI(title="Prompt Optimization Platform API")

@app.get("/health", tags=["Health"])
async def health_check():
    return {"status": "ok"}

app.include_router(api_router, prefix="/api/v1")
```

Implement API endpoints following the specification in `APIinfo.md`:

```python
# src/api/endpoints/prompts.py
from fastapi import APIRouter, HTTPException

router = APIRouter(prefix="/prompts", tags=["Prompts"])

@router.post("/")
async def create_prompt(prompt_data: dict):
    # Implementation
    
@router.get("/{prompt_id}")
async def get_prompt(prompt_id: str):
    # Implementation

# Additional endpoints...
```

## Advanced ML Extensions

For the meta-learning and reinforcement learning components:

1. **Meta-Learning Predictor**
   - Implement `train_meta_model` task to predict prompt quality before expensive LLM calls
   - Use scikit-learn's `RandomForestRegressor` or similar models

2. **Reinforcement Learning Agent**
   - Implement custom Gym environment in `custom_env.py`
   - Use Stable-Baselines3 PPO algorithm for prompt optimization

## Next Steps

After implementing the framework:

1. **Complete Task Implementations**: Fill in the actual logic for each task and flow.
2. **Add Tests**: Create unit tests for the core functionality.
3. **API Documentation**: Implement Swagger/OpenAPI docs for the API endpoints.
4. **Monitoring**: Add telemetry and metrics tracking.
5. **Deployment**: Set up CI/CD pipeline for deployment.

## Dependencies

```
fastapi~=0.100.0
uvicorn[standard]~=0.23.2
pydantic~=2.0
python-dotenv~=1.0.0
prefect~=2.13.0
google-cloud-aiplatform~=1.36.0
evaluate~=0.4.0
pandas~=2.0
scikit-learn~=1.3
stable-baselines3[extra]~=2.0
wandb~=0.16.0
gcsfs
joblib
```

---

This implementation follows the architectural patterns from:
- `BACKEND_IMPLEMENTATION_FRAMEWORK.md`
- `APIinfo.md`
- `API_ARCHITECTURE_README.md`
- `ML backbone`
- `ML_PREFECT_PIPELINE.md` 
Analysis of Existing Prompts:
The current system prompt relies heavily on setting an idealized persona for the LLM ("expert physician," "100% on all exams," etc.). While this might seem helpful, it's ultimately ineffective and can lead to overconfidence and hallucination.  The extensive restriction module, while well-intentioned, is presented in a way that doesn't integrate smoothly with the reasoning process. The examples within the restriction module are few-shot learning examples that are essentially a form of hard-coding, violating the "domain-general" requirement. The current output prompt is overly structured and prescriptive, potentially stifling natural reasoning and leading to formulaic responses rather than insightful ones. The N-shot learning examples in the output prompt, while aiming to improve reasoning, are lengthy and again risk memorization rather than generalization of reasoning strategies.  The core problem is a lack of explicit guidance on *how* to reason, focusing instead on the desired persona and output format.  The examples show the model failing to extract the relevant information and apply its medical knowledge correctly; it gets bogged down in the details and doesn't synthesize them effectively.

The revised system prompt removes the unrealistic persona and focuses on the core task: medical reasoning. The restriction module is simplified and integrated more naturally into the instructions.  The revised output prompt encourages a structured, step-by-step approach to diagnosis, explicitly requiring the model to justify its choices and eliminate less likely diagnoses. This promotes transparency and reduces the likelihood of hallucinations.  The removal of lengthy examples prevents memorization and encourages the model to apply general medical reasoning principles. The focus is shifted from mimicking a perfect physician to performing a specific task (medical reasoning) in a structured manner. This approach encourages the model to focus on applying its knowledge rather than trying to meet an impossible standard of perfection. The new prompts are concise, domain-general, and directly address the identified reasoning deficiencies.